% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={GEOG0030: Geocomputation},
  pdfauthor={Justin van Dijk},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{GEOG0030: Geocomputation}
\author{Justin van Dijk}
\date{Last modified: 2022-12-08}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{module-overview}{%
\chapter*{Module Overview}\label{module-overview}}
\addcontentsline{toc}{chapter}{Module Overview}

\hypertarget{module-introduction}{%
\chapter*{Module Introduction}\label{module-introduction}}
\addcontentsline{toc}{chapter}{Module Introduction}

\begin{center}\includegraphics[width=1\linewidth]{images/general/geocomputation_welcome} \end{center}

\emph{Last modified: 2022-12-08}

\hypertarget{welcome}{%
\section*{Welcome}\label{welcome}}
\addcontentsline{toc}{section}{Welcome}

Welcome to \textbf{Geocomputation}. This module will introduce you both to the principles of spatial analysis as well as provide you with a comprehensive introduction to the use of programming. Over the next ten weeks, you will learn about the theory, methods and tools of spatial analysis through relevant case studies. We will start by using QGIS before moving to the R programming language. You will learn how to find, manage and clean spatial, demographic and socioeconomic datasets, and then analyse them using core spatial and statistical analysis techniques.

\hypertarget{moodle}{%
\section*{Moodle}\label{moodle}}
\addcontentsline{toc}{section}{Moodle}

\href{https://moodle.ucl.ac.uk/}{Moodle} is the central point of contact for GEOG0030 and it is where all important information will be communicated such as key module and assessment information. This workbook contains links to all reading material as well as the content of all computer tutorials

\hypertarget{module-overview-1}{%
\section*{Module overview}\label{module-overview-1}}
\addcontentsline{toc}{section}{Module overview}

The topics covered over the next ten weeks are:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1212}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3030}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5758}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Week
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Section
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Topic
\end{minipage} \\
\midrule()
\endhead
1 & Foundational Concepts & \href{geocomputation-an-introduction.html}{Geocomputation: An Introduction} \\
2 & Foundational Concepts & \href{giscience-and-gis-software.html}{GIScience and GIS software} \\
3 & Foundational Concepts & \href{cartography-and-visualisation.html}{Cartography and Visualisation} \\
4 & Foundational Concepts & \href{programming-for-data-analysis.html}{Programming for Data Analysis} \\
5 & Foundational Concepts & \href{programming-for-spatial-analysis.html}{Programming for Spatial Analysis} \\
& \textbf{Reading week} & \textbf{Reading week} \\
6 & Core Spatial Analysis & \href{analysing-spatial-patterns-i-geometric-operations-and-spatial-queries.html}{Analysing Spatial Patterns I: Geometric Operations and Spatial Queries} \\
7 & Core Spatial Analysis & \href{analysing-spatial-patterns-ii-spatial-autocorrelation.html}{Analysing Spatial Patterns II: Spatial Autocorrelation} \\
8 & Core Spatial Analysis & \href{analysing-spatial-patterns-iii-point-pattern-analysis.html}{Analysing Spatial Patterns III: Point Pattern Analysis} \\
9 & Advanced Spatial Analysis & \href{rasters-zonal-statistics-and-interpolation.html}{Rasters, Zonal Statistics and Interpolation} \\
10 & Advanced Spatial Analysis & \href{transport-network-analysis.html}{Transport Network Analysis} \\
\bottomrule()
\end{longtable}

\hypertarget{troubleshooting}{%
\section*{Troubleshooting}\label{troubleshooting}}
\addcontentsline{toc}{section}{Troubleshooting}

Spatial analysis can yield fascinating insights into geographical relationships, albeit at times it can be challenging, particularly when we combine this with learning how to program at the same time. You will most likely encounter many error messages, experience software crashes, and spend hours to identify bugs in your code. However, the rewards of learning how to programmatically solve complex spatial problems will be very much worth it in the end.

If you need specific assistance with this course please:

\begin{itemize}
\tightlist
\item
  Ask a question at the end of a lecture or during the computer practical.
\item
  Attend the Department's \textbf{Coding Therapy sessions} that are run on a weekly basis.
\item
  Check the \href{https://moodle.ucl.ac.uk/}{Moodle} assessment tab for queries relating to this module's assessment.
\end{itemize}

If after pursuing all these avenues you still need help, you can book into our office hours. You can use an office hour to discuss a geographical concept in relation to the material, assessment or for any personal matters relevant to the completion of the module.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

This year's workbook is updated and compiled using:

\begin{itemize}
\tightlist
\item
  The \href{https://jo-wilkin.github.io/GEOG0030/coursebook/index.html}{GEOG0030: Geocomputation 2021-2021} workbook as created and compiled by Dr Jo Wilkin.
\item
  The \href{https://jtvandijk.github.io/GEOG0030_20212022/}{GEOG0030: Geocomputation 2021-2022} workbook.
\end{itemize}

The datasets used in this workbook contain:

\begin{itemize}
\tightlist
\item
  Crime data obtained from \href{https://data.police.uk/}{data.police.uk} (Open Government Licence)
\item
  National Statistics data © Crown copyright and database right {[}2015{]} (Open Government Licence)
\item
  Ordnance Survey data © Crown copyright and database right {[}2015{]}
\item
  Public Health England © Crown copyright 2021
\end{itemize}

\hypertarget{foundational-concepts}{%
\chapter*{Foundational Concepts}\label{foundational-concepts}}
\addcontentsline{toc}{chapter}{Foundational Concepts}

\hypertarget{geocomputation-an-introduction}{%
\chapter{Geocomputation: An Introduction}\label{geocomputation-an-introduction}}

This week's lecture provided you with a thorough introduction on Geocomputation, outlining how and why it is different to a traditional GIScience course. We set the scene for the remainder of the module and explained how the foundational concepts that you will learn in the first half of term sit within the overall module. This week we start easy by setting up our work environment and set up the software that we will need over the coming weeks.

\hypertarget{reading-w01}{%
\section{Reading list}\label{reading-w01}}

\hypertarget{essential-readings}{%
\subsubsection*{Essential readings}\label{essential-readings}}
\addcontentsline{toc}{subsubsection}{Essential readings}

\begin{itemize}
\tightlist
\item
  Brundson, C. and Comber, A. 2020. Opening practice: Supporting reproducibility and critical spatial data science. \emph{Journal of Geographical Systems} 23: 477--496. \href{https://doi.org/10.1007/s10109-020-00334-2}{{[}Link{]}}
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& Systems, \textbf{Chapter 1}: \emph{Geographic Information: Science, Systems, and Society}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\item
  Singleton, A. and Arribas-Bel, D. 2019. Geographic Data Science. \emph{Geographical Analysis}. \href{https://doi.org/10.1111/gean.12194}{{[}Link{]}}
\end{itemize}

\hypertarget{suggested-readings}{%
\subsubsection*{Suggested readings}\label{suggested-readings}}
\addcontentsline{toc}{subsubsection}{Suggested readings}

\begin{itemize}
\tightlist
\item
  Miller, H. and Goodchild, M. 2015. Data-driven geography. \emph{GeoJournal} 80: 449--461. \href{https://doi.org/10.1007/s10708-014-9602-6}{{[}Link{]}}
\item
  Goodchild, M. 2009. Geographic information systems and science: Today and tomorrow. \emph{Annals of GIS} 15(1): 3-9. \href{https://doi.org/10.1080/19475680903250715}{{[}Link{]}}
\end{itemize}

\hypertarget{getting-started}{%
\section{Getting started}\label{getting-started}}

Over the next few weeks, we will be taking a closer look at many of the foundational concepts that will ultimately enable you to confidently and competently analyse spatial data using both programming and GIS software. You will further learn how to plan, structure and conduct your own spatial analysis using programming -- whilst making decisions on how to best present your work, which is a crucial aspect of any type of investigation but of particular relevance to your dissertation.

To help with this, we highly recommend that you try to stay organised with your work, including taking notes and making yourself a coding handbook. We would also suggest to list the different datasets you come across - and importantly, the scales and different projections you use them at - more on this over the next weeks. Finally, you should also make notes about the different spatial analysis techniques you come across, including the different properties they assess and parameters they require to run.

\hypertarget{software}{%
\section{Software}\label{software}}

This course primarily uses the \href{https://www.r-project.org/}{R} programming language, although we start by using \href{https://qgis.org/en/site/}{QGIS} in the next two weeks to give you a basic foundation in the principles of spatial analysis.

\textbf{Note}
Please follow the instructions below to install both \href{https://www.r-project.org/}{R} and \href{https://qgis.org/en/site/}{QGIS} onto your own personal computer. If you cannot install the software on your personal computer or you are not planning to bring your own laptop to the computer practicals, please refer to the \protect\hyperlink{ucl}{UCL Desktop and RStudio Server} section below. Please make sure that you have access to a working installation of QGIS and R (including relevant packages) \textbf{before} the first hands-on practical session next week.

\hypertarget{qgis-installation}{%
\subsection{QGIS Installation}\label{qgis-installation}}

QGIS is an open-source graphic user interface GIS with many community developed add-on packages (or plugins) that provide additional functionality to the software. You can download and install QGIS on your personal machine by going to the QGIS website: \href{https://qgis.org/en/site/forusers/download.html}{{[}Link{]}}.

\textbf{Note}
We recommend installing the \textbf{Long Term Release} (\emph{QGIS 3.22 LTR}) as this version should be the most stable version. For Windows users: the QGIS installation may be a little slow.

After installation, start QGIS to see if the installation was successful and no errors are shown after start up.

\hypertarget{r-and-rstudio-installation}{%
\subsection{R and RStudio Installation}\label{r-and-rstudio-installation}}

R is both a programming language and software environment - in the form of RStudio- originally designed for statistical computing and graphics. R's great strength is that it is open-source, can be used on any computer operating system, and is free for anyone to use and contribute to. Because of this, it is rapidly becoming the statistical language of choice for many academics and has a very large user community with people constantly contributing new packages to carry out all manner of statistical, graphical, and importantly for us, geographical tasks.

Installing R takes a few relatively simple steps involving two programmes. First there is the R programme itself. Follow these steps to get it installed on your computer:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Navigate in your browser to your nearest CRAN mirror: \href{https://cran.ma.imperial.ac.uk/}{{[}Link{]}}
\item
  If you use a Windows computer, click on \emph{Download R for Windows}. Then click on \emph{base}. Download and install \textbf{R 4.2.x for Windows}. If you use a Mac computer, click on \emph{Download R for macOS} and download and install \textbf{R-4.2.x.pkg}
\end{enumerate}

That is it! You now have installed the latest version of R on your own machine. However, to make working with R a little bit easier we also need to install something called an Integrated Development Environment (IDE). We will use RStudio:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Navigate to the official webpage of RStudio: \href{https://posit.co/download/rstudio-desktop/\#download}{{[}Link{]}}
\item
  Download and install RStudio Desktop on your computer (\textbf{free version!})
\end{enumerate}

After this, start RStudio to see if the installation was successful and no errors are shown after start up.

\hypertarget{ucl}{%
\subsection{UCL Desktop and RStudio Server}\label{ucl}}

As an alternative to installing QGIS and R with RStudio onto your personal device, there are some other options. Firstly, both programmes are available through \href{https://www.ucl.ac.uk/isd/services/computers/remote-access/desktopucl-anywhere}{Desktop@UCL Anywhere} as well as all UCL computers on campus. In case of R, there is also an RStudio server version available which you can access through your web browser: \href{https://rstudio.data-science.rc.ucl.ac.uk/}{{[}Link{]}}

You should be able to log in with your normal UCL username and password. After logging in, you should see the RStudio interface appear.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w01/rstudio_interface} 

}

\caption{The RStudio Server interface.}\label{fig:01-rstudio-interface}
\end{figure}

\textbf{Note}
If it is the first time you log on to RStudio server you may only see the RStudio interface appear once you have clicked on the \emph{start a new session} button. More importantly: if you are not on campus, RStudio server will only work with an active Virtual Private Network (VPN) connection that links your personal computer into UCL's network. Details on setting up a VPN connection can be found in UCL's VPN connection guides: \href{https://www.ucl.ac.uk/isd/services/get-connected/ucl-virtual-private-network-vpn}{{[}Link{]}}

\hypertarget{r-package-installation}{%
\subsection{R package installation}\label{r-package-installation}}

Now we have installed or have access to QGIS and R, we need to customise R. Many useful R function come in packages, these are free libraries of code written and made available by other by R users. This includes packages specifically developed for data cleaning, data wrangling, visualisation, mapping, and spatial analysis. To save us some time, we will install all R packages that we will need over the next ten weeks in one go. Now copy and paste the following code into the \textbf{console}. You can execute the code by hitting \textbf{Enter}. This may take a while.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install all packages that we need}
\FunctionTok{install.packages}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}tidyverse\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}sf\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}tmap\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}osmdata\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}RColorBrewer\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}janitor\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}spdep\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}dbscan\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}raster\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}spatstat\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}gstat\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}dodgr\textquotesingle{}}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Once you have installed the packages, we need to check whether we can in fact load them into our R session. Copy and paste the following code into the \textbf{console}, and executed by hitting \textbf{Enter} again.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load all packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(tmap)}
\FunctionTok{library}\NormalTok{(osmdata)}
\FunctionTok{library}\NormalTok{(RColorBrewer)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(spdep)}
\FunctionTok{library}\NormalTok{(dbscan)}
\FunctionTok{library}\NormalTok{(raster)}
\FunctionTok{library}\NormalTok{(spatstat)}
\FunctionTok{library}\NormalTok{(gstat)}
\FunctionTok{library}\NormalTok{(dodgr)}
\end{Highlighting}
\end{Shaded}

You will see some information printed to your console but as long as you do not get a message that is similar to \texttt{Error:\ package\ or\ namespace\ load\ failed\ for\ \textless{}packagename\textgreater{}} or \texttt{Error:\ package\ \textquotesingle{}\textless{}packagename\textquotesingle{}\ could\ not\ be\ loaded} all should be fine.

\textbf{Note}
Even if you have used R or RStudio Server before and already installed some of the packages in the above list, do re-install all packages to make sure you have the latest versions. Legacy installations that have not been updated may lay lead to problems when going through the tutorials.

\hypertarget{a-note-on-arcgis}{%
\subsection{A note on ArcGIS}\label{a-note-on-arcgis}}

\href{https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview}{ArcGIS Pro} (previously ArcMap) is the main commercial GIS software that you may have already used - or seen/heard about through other modules or even job adverts. We do not use ArcGIS Pro in our Practicals for several reasons:

\begin{itemize}
\tightlist
\item
  Computing requirements for ArcGIS Pro are substantial and it \textbf{only} operates on the Windows Operating System. For Mac users, using ArcGIS Pro (and ArcMap) would require using either a Virtual Machine or running a copy of Windows OS on a separate partition of your hard drive.
\item
  It is \textbf{proprietary} software, which means you need a license to use the software. For those of us in education, the University covers the cost of this license, but when you leave, you will need to pay for a personal license (around £100 for non-commercial use) to continue using the software and repeat any analysis you have used the software for.
\item
  Whilst ArcPro can use pure \href{https://www.python.org/}{Python} (and even R) as a programming language within it through scripts and notebooks, it primarily relies on its own \textbf{ArcPy} and \textbf{ArcGIS API for Python} packages to run the in-built tools and analytical functions. To use these packages, you still need a license which makes it difficult to share your code with others \emph{if} they do not have their own ArcGIS license.
\end{itemize}

Recent developments in the ArcPro software, however, does make it an attractive tool for spatial data science and quantitative geography - it has cross-user functionality, from data analysts who like to use a tool called Notebooks for their code development, to those focused more on cartography and visualisation with in-built bridges to Adobe's Creative Suite. We therefore do not want to put you off looking into ArcGIS in the future, but for this course, we want to ensure the reproducibility of your work.

\textbf{Note} This also means that the analysis you will be doing for your coursework assignment must be completed in R and QGIS. Specific guidance on the coursework assignment and permitted software will be made available at the end of Reading Week.

\hypertarget{byl-w01}{%
\section{Before you leave}\label{byl-w01}}

You should now be all ready to go with the computer practicals the coming week. \href{https://www.youtube.com/watch?v=d8Fmu3RLEOY}{That is it for this week}!

\hypertarget{giscience-and-gis-software}{%
\chapter{GIScience and GIS software}\label{giscience-and-gis-software}}

This week's lecture introduced you to foundational concepts associated with GIScience and GIS software, with particular emphasis on the representation of spatial data and sample design. Out of all our foundational concepts you will come across in the next four weeks, this is probably the most substantial to get to grips with and has both significant theoretical and practical aspects to its learning. The practical component of the week puts some of these learnings into practice, starting with a short digitisation excercise followed by a simple visualisation of London's population over time.

\hypertarget{reading-w02}{%
\section{Reading list}\label{reading-w02}}

\hypertarget{essential-readings-1}{%
\subsubsection*{Essential readings}\label{essential-readings-1}}
\addcontentsline{toc}{subsubsection}{Essential readings}

\begin{itemize}
\tightlist
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& Systems, \textbf{Chapter 2}: \emph{The Nature of Geographic Data}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& Systems, \textbf{Chapter 3}: \emph{Representing Geography}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& Systems, \textbf{Chapter 7}: \emph{Geographic Data Modeling}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\end{itemize}

\hypertarget{suggested-readings-1}{%
\subsubsection*{Suggested readings}\label{suggested-readings-1}}
\addcontentsline{toc}{subsubsection}{Suggested readings}

\begin{itemize}
\tightlist
\item
  Goodchild, M. and Haining, R. 2005. GIS and spatial data analysis: Converging perspectives. \emph{Papers in Regional Science} 83(1): 363--385. \href{https://doi.org/10.1007/s10110-003-0190-y}{{[}Link{]}}
\item
  Schurr, C., Müller, M. and Imhof, N. 2020. Who makes geographical knowledge? The gender of Geography's gatekeepers. \emph{The Professional Geographer} 72(3): 317-331. \href{https://doi.org/10.1080/00330124.2020.1744169}{{[}Link{]}}
\item
  Yuan, M. 2001. Representing complex geographic phenomena in GIS. \emph{Cartography and Geographic Information Science} 28(2): 83-96. \href{https://doi.org/10.1559/152304001782173718}{{[}Link{]}}
\end{itemize}

\hypertarget{simple-digitisation-of-spatial-features}{%
\section{Simple digitisation of spatial features}\label{simple-digitisation-of-spatial-features}}

To get spatial features in a digital form, they need to be digitised. Let's take what should be a straight-forward example of digitising the River Thames in London.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w02/river_thames} 

}

\caption{The Thames.}\label{fig:02-digitise}
\end{figure}

We are going to use a very simple online tool that allows us to create digital data and export the data we create as raw files.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Head to \href{http://geojson.io/\#map=12/51.5170/-0.1030}{geojson.io}.
\item
  In the bottom left-hand corner, select \emph{Satellite Streets} as your map option.
\item
  Next, click on the \texttt{Draw\ Linestring} tool which you can find on the right hand side of the screen. You can hover over the icons to get the names of each tool.
\item
  Now digitise the river Thames. Simply click from a starting point on the left- or right-hand side of the map, and digitise the river.
\item
  Once you are done, double-click your final point to end your line.
\item
  You can click on the line and select \emph{Info} in the pop-up screen to find out how long the line is.
\item
  You can export your data using the \emph{Save} menu.
\end{enumerate}

\hypertarget{questions}{%
\subsubsection*{Questions}\label{questions}}
\addcontentsline{toc}{subsubsection}{Questions}

\begin{itemize}
\tightlist
\item
  How easy did you find it to digitise the data and what decisions did you make in your own ``sample scheme''?
\item
  How close together are your clicks between lines?
\item
  Did you sacrifice detail over expediency or did you spend perhaps a little too long trying to capture ever small bend in the river?
\item
  How well do you think your line represents the River Thames?
\end{itemize}

\hypertarget{population-change-in-london}{%
\section{Population change in London}\label{population-change-in-london}}

The second part of this practical will introduces you to \textbf{attribute joins} followed by creating a \href{https://en.wikipedia.org/wiki/Choropleth_map}{choropleth map}. You will be using different types of \emph{joins} throughout this module, and probably the rest of your career, so it is incredibly important that you understand how they work.

\textbf{Note}
The datasets you will create in this practical will be used in next week's practical, so make sure to follow every step and save your data carefully.

When using spatial data, there is generally a very specific workflow that you will need to go through and, believe it or not, the majority of this is not actually focused on analysing your data. Along with the idea that 80\% of data is geographic data, the second most often-quoted GIS-related unreferenced `fact' is that anyone working with spatial data will spend 80\% of their time simply finding, retrieving, managing and processing the data before any analysis can be done.

One of the reasons behind this need for a substantial amount of processing is that the data you often need to use is almost never in the format that you require for analysis. For example, for our investigation, there is not a `ready-made' spatial population dataset (i.e.~population \texttt{shapefile}) we can download to explore population change across England:

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w02/datasearch} 

}

\caption{Alas a quick Google search shows that finding a shapefile of England's population is not straightforward.}\label{fig:02-google-shp}
\end{figure}

Instead, we need to go and find the raw datasets and create the data layers that we want. As a result, before beginning any spatial analysis project, it is best-practice to think through what end product you will ultimately need for your analysis.

A typical spatial analysis workflow usually looks something like this:

\begin{itemize}
\tightlist
\item
  \textbf{Identify} the data you need to complete your analysis i.e.~answer your research questions. This includes thinking through the scale, coverage and currency of your dataset.
\item
  \textbf{Find} the data that matches your requirements, e.g.~is it openly and easily available?
\item
  \textbf{Download} the data and store it in the correct location.
\item
  \textbf{Clean} the data. This may be done before or after ingesting your data into your chosen software programme.
\item
  \textbf{Load} the data into your chosen software programme.
\item
  \textbf{Transform and process} the data. This may require re-projection, creating joins between datasets, calculating new fields and applying selections.
\item
  \textbf{Analyse} your data using appropriate methods.
\item
  \textbf{Visualise} your data and results with graphs and maps.
\item
  \textbf{Communicate} your results.
\end{itemize}

As you can see, the analysis and visualisation part comes quite late in the overall spatial analysis workflow - and instead, the workflow is very top-heavy with data management. However, very often in GIS-related courses you will be given pre-processed datasets. Because data management is an essential part of your workflow, we are clean (the majority of) our data from the get-go. This will help you understand the processes that you will need to go through in the future as you search for and download your own data, as well as deal with the data first-hand before loading it into our GIS software.

\hypertarget{w02-scene}{%
\subsection{Setting the scene}\label{w02-scene}}

For this practical, we will investigate how the population in London has changed over time. Understanding population change - over time and space - is spatial analysis at its most fundamental. We can understand a lot just from where population is growing or decreasing, including thinking through the impacts of these changes on the provision of housing, education, health and transport infrastructure.

We can also see first-hand the impact of wider socio-economic processes, such as urbanisation. Today we will look at population in London in 2011, 2015, and 2019 at the \emph{Ward} scale that we can use within our future analysis projects, starting next week.

\textbf{Note}
We will use the population dataset to \emph{normalise} other datasets. Why? When we record events created by humans, there is often a population bias: simply, more people in an area will by probability lead to a higher occurrence of said event, such as crime. We will look at this in greater detail next week.

\hypertarget{w02-finding}{%
\subsection{Finding data}\label{w02-finding}}

In the UK, finding authoritative data on population and \emph{Administrative Geography} boundaries is increasingly straight-forward. Over the last decade, the UK government has opened up many of its datasets as part of an \textbf{Open Data} precedent that began in 2010 with the creation of \url{data.gov.uk} and the Open Government Licence (the terms and conditions for using data).

\href{www.data.gov.uk}{Data.gov.uk} is the UK government's central database that contains open data that the central government, local authorities and public bodies publish. This includes, for example, aggregated census and health data -- and even government spending. In addition to this central database, there are other authoritative databases run by the government and/or respective public bodies that contain either a specific type of data (e.g.~census data, crime data) or a specific collection of datasets (e.g.~health data from the NHS, data about London). Some portals are less up-to-date than others, so it is wise to double-check with the `originators' of the data to see if there are more recent versions.

For our practical, we will access data from two portals:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For our administrative boundaries, we will download the \textbf{spatial} data from the \emph{London Datastore} (which is exactly what it sounds like).
\item
  For population, we will download \textbf{attribute} data from the \emph{Office of National Statistics (ONS)}.
\end{enumerate}

\hypertarget{w01-housekeeping}{%
\subsection{Housekeeping}\label{w01-housekeeping}}

Before we download our data, it is important to establish an organised file systems that we will use throughout the module:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create a \texttt{GEOG0030} folder in your \texttt{Documents} folder on your computer.
\item
  Within your \texttt{GEOG0030} folder, create the following subfolders:
\end{enumerate}

\begin{longtable}[]{@{}ll@{}}
\toprule()
Folder name & Purpose \\
\midrule()
\endhead
\texttt{data} & To store both raw data sets and final outputs. \\
\texttt{maps} & To save the maps you produce during your tutorials. \\
\bottomrule()
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Within your \texttt{data} folder, create the following subfolders:
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Folder name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule()
\endhead
\texttt{raw} & To store all your raw data files that have not yet been processed. \\
\texttt{output} & To store all your final data files that have been processed and analysed, potentially ready to be mapped. \\
\bottomrule()
\end{longtable}

\hypertarget{w02-downloading}{%
\subsection{Downloading data}\label{w02-downloading}}

We will start by downloading the administrative geography boundaries:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Navigate to the relevant page on the London Datastore: \href{https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london}{{[}Link{]}}.
\item
  Download all three zipfiles to your computer: \texttt{statistical-gis-boundaries-london.zip}, \texttt{London-wards-2014.zip} and \texttt{London-wards-2018.zip}.
\end{enumerate}

The first dataset contains all levels of London's administrative boundaries. In descending size order: Borough, Ward, Middle layer Super Output Area (MSOA), Lower layer Super Output Area (LSOA), and Output Area (OA) based on the 2011 Census. The second dataset contains an \emph{updated} version of the Ward boundaries, as of 2014. The third dataset contains yet another \emph{updated} version of the Ward boundaries, as of 2018. As we will be looking at population data for 2015 and 2019, it is best practice to use those boundaries that are most reflective of the `geography' at the time; therefore, we will use these 2014 / 2018 Ward boundaries for our 2015 / 2019 population dataset, respectively.

\textbf{Note}
Once downloaded, you will need to unzip all files before you can use them. To unzip the file, you can use the built-in functionality of your computer's operating system. For Windows: right click on the zip file, select \textbf{Extract All}, and then follow the instructions. For Mac OS: double-click on the the zip file and it should unzip automatically.

Once unzipped, you will find two folders: \emph{Esri} and \emph{MapInfo.} These folders contain the same data but in different data formats: \textbf{Esri shapefile} and \textbf{MapInfo TAB}.

\textbf{Note}
MapInfo is another proprietary GIS software, which has historically been used in public sectors services in the UK and abroad, although has generally been replaced by either Esri's ecosystem or open-source software GIS.

Now open your \texttt{GEOG0030/data/raw/} folder and create a new folder called \texttt{boundaries}. Within this folder, create three new folders: \texttt{2011}, \texttt{2014} and \texttt{2018}. Copy the entire contents of \texttt{Esri} folder of each year into their respective year folder.

We do not want to add the additional \texttt{Esri} folder as a step in our filesystem, i.e.~your file paths should read: \texttt{GEOG0030/data/raw/boundaries/2011} for the 2011 boundaries, \texttt{GEOG0030/data/raw/boundaries/2014} for the 2014 boundaries, and \texttt{GEOG0030/data/raw/boundaries/2018} for the 2018 boundaries.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w02/file_management} 

}

\caption{Your setup should look something like this.}\label{fig:02-setup}
\end{figure}

We now have our administrative geography files ready for use.

\textbf{Note}
Administrative geographies are a way of dividing the country into smaller sub-divisions or areas that correspond with the area of responsibility of local authorities and government bodies. These administrative sub-divisions and their associated geography have several important uses, including assigning electoral constituencies, defining jurisdiction of courts, planning public healthcare provision, as well as what we are concerned with: used as a mechanism for collecting census data and assigning the resulting datasets to a specific administrative unit. These geographies are updated as populations evolve and as a result, the boundaries of the administrative geographies are subject to either periodic or occasional change. The UK has quite a complex administrative geography, particularly due to having several countries within one overriding administration and then multiple ways of dividing the countries according to specific applications. More details on the administrative geographies of the UK can be found on the website of the \href{https://www.ons.gov.uk/methodology/geography/ukgeographies/administrativegeography}{Office for National Statistics}.

For our population datasets, we will use the ONS mid-year estimates (MYE). These population datasets are estimates that are based on the 2011 census count and then updated with estimated population growth. They are released once a year, with a delay of a year. Today we will use the data for 2011, 2015, and 2019.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Navigate to the \emph{Ward} level datasets: \href{https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/wardlevelmidyearpopulationestimatesexperimental}{{[}Link{]}}
\item
  When you navigate to this page, you will find multiple choices of data to download. We will need to download the estimates for \textbf{2011, 2015} and \textbf{2019}. Click to download each of the zipfiles. Choose the \textbf{revised} versions for 2015 and the (Census-based) 2011 Wards edition for 2011.
\item
  In your \texttt{GEOG0030/data/raw/} folder, create a new folder called \texttt{population}, unzip your downloaded files, and copy the three spreadsheets to the newly created \texttt{population} folder.
\item
  Rename the files you donwloaded to: \texttt{MYE\_ward\_2011.xls}, \texttt{MYE\_ward\_2015.xls}, and \texttt{MYE\_ward\_2019.xlsx}.
\end{enumerate}

Now it is time to do some quite extensive data cleaning and preparation.

\hypertarget{w02-cleaning}{%
\subsection{Cleaning data}\label{w02-cleaning}}

When you open up any of the Ward spreadsheets in Excel =, you will notice that there are several worksheets contained in this workbook. However, we are only interested in the total population tab. We therefore need to copy over the data from the 2011, 2015 and 2019 datasets into separate \texttt{csv} files.

\hypertarget{london-population-in-2011}{%
\subsubsection{London population in 2011}\label{london-population-in-2011}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open the 2011 Ward spreadsheet in Excel.
\item
  Click on the \texttt{Mid-2011\ Persons} tab and have a look at the data. As you should be able to see, we have a set of different fields (e.g.~\texttt{Ward\ Code}, \texttt{Ward\ Name}), including population counts. Because we do not need all the data in the spreadsheet, we will extract only the data we need for our analysis. This means we need the total population (\texttt{All\ Ages}) data, alongside some identifying information that distinguishes each record from one another. Here we can see that both \texttt{Ward\ Code} and \texttt{Ward\ Name} suit this requirement. We can also think that the \texttt{Local\ Authority} column might be of use, so we also keep this information.
\item
  Create a new Excel spreadsheet Excel and from the \texttt{Mid-2011\ Persons} spreadsheet, copy over all cells from columns \textbf{A} to \textbf{D} and rows \textbf{4 to 636} into this new spreadsheet. Row 636 denotes the end of the \emph{Greater London} Wards (i.e.~the end of the \emph{Westminster Local Authority}) which are kept (in most scenarios) at the top of the spreadsheet as their \textbf{Ward Codes} are the first in sequential order.
\item
  Before we go any further, we need to format our data. First, we want to rename our fields to remove the spaces and superscript formatting. Re-title the fields as follows: \texttt{ward\_code}, \texttt{ward\_name}, \texttt{local\_authority} and \texttt{pop2011}.
\item
  One further bit of formatting that you must do before saving your data is to format our population field. At the moment, you will see that there are commas separating the thousands within our values. If we leave these commas in our values, QGIS will read them as decimal points, creating decimal values of our population. There are many points at which we could solve this issue, but the easiest point is now - we will strip our population values of the commas and set them to integer (whole numbers) values. To format the \texttt{pop2011} column, select the entire column and right-click on the \texttt{D} cell. Click on \textbf{Format Cells} and set the Cells to \textbf{Number} with \textbf{0} decimal places. You should see that the commas are now removed from your population values.
\item
  Save your spreadsheet into your \texttt{output} folder as \texttt{ward\_population\_2011.csv}.
\end{enumerate}

\hypertarget{london-population-in-2015}{%
\subsubsection{London population in 2015}\label{london-population-in-2015}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open the 2015 Ward spreadsheet in Excel.
\item
  As you will see again, there are plenty of worksheets available and we want to select the \texttt{Mid-2015\ Persons} tab. We now need to copy over the data from our 2015 dataset to a new spreadsheet again. However, at first instance, you will notice that the City of London (CoL) Wards are missing from this dataset. Then if you scroll to the end of the London Local Authorities, i.e.~to the bottom of Westminster, what you should notice is that the final row for the Westminster data is in fact row 575 - this suggests we are missing the data fror some Local Authorities (LAs). We need to determine which ones are missing and try to find them in the 2015 spreadsheet. With this in mind, start by copying over all cells from columns \textbf{A} to \textbf{D} and rows \textbf{5 to 575} into a new spreadsheet.
\item
  If you were to compare the names of the London Boroughs that we have now copied with the full list, you would notice that we are missing \emph{City of London}, \emph{Hackney}, \emph{Kensington and Chelsea}, and \emph{Tower Hamlets}. If we head back to the original 2015 raw dataset, we can actually find this data (as well as the City of London) further down in the spreadsheet. It seems like these LAs had their codes revised in the 2014 revision and are no longer in the same order as the 2011 dataset.
\item
  Locate the data for the \emph{City of London}, \emph{Hackney}, \emph{Kensington and Chelsea} and \emph{Tower Hamlets} and copy this over into our new spreadsheet. Double-check that you now have in total \textbf{637} Wards within your dataset.
\item
  Remember to rename the fields as above, but change your population field to \textbf{pop2015}. Also, remember to reformat the values in your \texttt{pop2015} column.
\item
  Once complete, save your spreadsheet into your \texttt{output} folder as \texttt{ward\_population\_2015.csv}.
\end{enumerate}

\hypertarget{london-population-in-2019}{%
\subsubsection{London population in 2019}\label{london-population-in-2019}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open the 2019 Ward spreadsheet in Excel. This time we are interested in the \texttt{Mid-2019\ Persons} tab.
\item
  This time the data that we are interested in can be found in columns \texttt{A}, \texttt{B}, \texttt{D} and \texttt{G}. Because the columns that we want are not positioned next to one another, start by hiding columns \texttt{C}, \texttt{E} and \texttt{F}. You can do this by right-clicking on the colums you want to hide and selecting \textbf{Hide}.
\item
  Next, copy the data from \textbf{row 5 to the final row for the Westminster data} for columns \texttt{A}, \texttt{B}, \texttt{D} and \texttt{G} over into a new spreadsheet.
\item
  If you look at the total rows that we have copied over, we have even fewer Wards than the 2015 dataset. This time we are not only missing data for \emph{City of London}, \emph{Hackney}, \emph{Kensington and Chelsea}, \emph{Tower Hamlets} but also for \emph{Bexley}, \emph{Croydon}, \emph{Redbridge}, and \emph{Southwark}.
\item
  Copy over the remaining Wards for these Local Authorities/Boroughs.
\item
  Once you've copied them over - you should now have \textbf{640} Wards. Delete columns \texttt{C}, \texttt{E} and \texttt{F} and rename the remaining fields as you have done previously. Also, remember to reformat the values in your \texttt{pop2019} column.
\item
  Once complete, save your spreadsheet into your \texttt{output} folder as \texttt{ward\_population\_2019.csv}.
\end{enumerate}

You should now have your three population \texttt{csv} datasets in your \texttt{output} folder. We are now (finally) ready to start using our data within QGIS.

\hypertarget{using-qgis-to-map-our-population-data}{%
\subsection{Using QGIS to map our population data}\label{using-qgis-to-map-our-population-data}}

\hypertarget{setting-up-a-project}{%
\subsubsection{Setting up a project}\label{setting-up-a-project}}

We will now use QGIS to create population maps for the Wards in London across our three time periods. To achieve this, we need to \textbf{join our table data to our spatial datasets} and then map our populations for our visual analysis.

Because, as we have seen above, we have issues with the number of Wards and changes in boundaries across our three years, we will not (for now) complete any quantitative analysis of these population changes - this would require significant additional processing that we do not have time for today.

\textbf{Note}
Data interoperability is a key issue that you will face in spatial analysis, particularly when it comes to Administrative Geographies.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start \textbf{QGIS}.
  Let's start a new project.
\item
  Click on \textbf{Project} -\textgreater{} \textbf{New}. Save your project as \texttt{w2-pop-analysis}. Remember to save your work throughout the practical.
\item
  Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on \textbf{Project} -\textgreater{} \textbf{Properties -- CRS}. In the Filter box, type \textbf{British National Grid}. Select \textbf{OSGB 1936 / British National Grid - EPSG:27700} and click Apply. Click \textbf{OK}.
\end{enumerate}

\textbf{Note}
We will explain CRSs and using CRSs in GIS software in more detail next week.

\hypertarget{adding-layers}{%
\subsubsection{Adding layers}\label{adding-layers}}

We will first focus on loading and joining the 2011 datasets.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on \textbf{Layer} -\textgreater{} \textbf{Add Layer} -\textgreater{} \textbf{Add Vector Layer}.
\item
  With \textbf{File} select as your source type, click on the small three dots button and navigate to your 2011 boundary files.
\item
  Here, we will select the \texttt{London\_Ward.shp} dataset. Click on the \texttt{.shp} file of this dataset and click \textbf{Open}. Then click \textbf{Add}. You may need to close the box after adding the layer.
\end{enumerate}

We can take a moment just to look at our Ward data - and recognise the shape of London. Can you see the City of London in the dataset? It has the smallest Wards in the entire London area. With the dataset loaded, we can now explore it in a little more detail. We want to check out two things about our data: first, its \textbf{Properties} and secondly, its \textbf{Attribute Table}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Right-click on the \texttt{London\_Ward} layer and open the \textbf{Attribute Table} and look at how the attributes are stored and presented in the table. Explore the different buttons in the Attribute Table and see if you can figure out what they mean. Once done, close the Attribute Table.
\item
  Right-click on the \texttt{London\_Ward} layer and select \textbf{Properties}. Click through the different tabs and see what they contain. Keep the \textbf{Properties} box open.
\end{enumerate}

Before adding our population data, we can make a quick map of the Wards in London - we can add labels and change the \emph{symbolisation} of our Wards.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  In the \textbf{Properties} box, click on the \textbf{Symbology} tab - this is where we can change how our data layer looks. For example, here we can change the line and fill colour of our Wards utilising either the default options available or clicking on \textbf{Simple Fill} and changing these properties directly. Keep the overall \textbf{styling} to a \textbf{Single Symbol} for now - we will get back to this once we have added the population data. You can also click on the \textbf{Labels} tab - and set the Labels option to \textbf{Single labels}.
\item
  QGIS will default to the \textbf{NAME} column within our data. You can change the properties of these labels using the options available. Change the font to \textbf{Futura} and size \textbf{8} and under the add a small buffer to the labels by selecting \textbf{Draw text bufer} under the \textbf{Buffer} tab. You can click \textbf{Apply} to see what your labels look like. Please note that the background colour may differ.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=31.31in]{images/w02/wardlabels} 

}

\caption{It looks incredibly busy.}\label{fig:02-busy-labels}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Click \textbf{OK} once you are done changing the Symbology and Label style of your data to return to the main window.
\end{enumerate}

\textbf{Note}
The main strength of a GUI GIS system is that is really helps us understand how we can visualise spatial data. Even with just these two shapefiles loaded, we can understand two key concepts of using spatial data within GIS.

The first, and this is only really relevant to GUI GIS systems, is that each layer can either be turned on or off, to make it visible or not (try clicking the tick box to the left of each layer). This is probably a feature you are used to working with if you have played with interactive web mapping applications before!

The second concept is the order in which your layers are drawn -- and this is relevant for both GUI GIS and when using plotting libraries such as \texttt{ggplot2} or \texttt{tmap} in RStudio. Your layers will be drawn depending on the order in which your layers are either tabled (as in a GUI GIS) or `called' in your function in code.

Being aware of this need for `order' is important when we shift to using RStudio and \texttt{tmap} to plot our maps, as if you do not layer your data correctly in your code, your map will end up not looking as you hoped!

For us using QGIS right now, the layers will be drawn from bottom to top. At the moment, we only have one layer loaded, so we do not need to worry about our order right now - but as we add in our 2015 and 2018 Ward files, it is useful to know about this order as we will need to display them individually to export them at the end.

\hypertarget{conducting-an-attribute-join}{%
\subsubsection{Conducting an attribute join}\label{conducting-an-attribute-join}}

We are now going to join our 2011 population data to our 2011 shapefile. First, we need to add the 2011 population data to our project.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on \textbf{Layer} -\textgreater{} \textbf{Add Layer} -\textgreater{} \textbf{Add Delimited Text Layer}.
\item
  Click on the three dots button again and navigate to your \textbf{2011 population data} in your \texttt{working} folder. Your file format should be set to \texttt{csv}. You should have the following boxes clicked under the \textbf{Record and Field options} menu: \texttt{Decimal\ separator\ is\ comma}, \texttt{First\ record\ has\ field\ names}, \texttt{Detect\ field\ types} and \texttt{Discard\ empty\ fields}. QGIS does many of these by default, but do double-check!
\item
  Set the Geometry to \emph{No geometry (attribute only table)} under the \textbf{Geometry Definition} menu. Then click \textbf{Add} and \textbf{Close}. You should now see a table added to your \texttt{Layers} box.
\end{enumerate}

We can now join this table data to our spatial data using an \textbf{Attribute Join}.

\textbf{Note}
An attribute join is one of two types of data joins you will use in spatial analysis (the other is a spatial join, which we will look at later on in the module). An attribute join essentially allows you to join two datasets together, as long as they share a common attribute to facilitate the `matching' of rows:

\begin{center}\includegraphics[width=12.08in]{images/w02/attribute_joins} \end{center}

Essentially you need a \textbf{single identifying ID} field for your records within both datasets: this can be a code, a name or any other string of information. In spatial analysis, we always \textbf{join our table data to our shape data} (One way to think about it as attaching the table data to each shape).

As a result, your target layer is always the shapefile (or spatial data) whereas your join layer is the table data. These are known as the left- and right-side tables when working with code.

\textbf{To make a join work, you need to make sure your ID field is correct across both datasets}, i.e.~no typos or spelling mistakes. Computers can only follow instructions, so they do not know that \emph{St.~Thomas} in one dataset is that same as \emph{St Thomas} in another, or even \emph{Saint Thomas}! It will be looking for an exact match!

As a result, whilst in our datasets we have kept both the name and code for both the boundary data and the population data, \textbf{when creating the join, we will always prefer to use the CODE over their names}. Unlike names, codes reduce the likelihood of error and mismatch because they do not rely on understanding spelling!

Common errors, such as adding in spaces or using \texttt{0} instead \texttt{O} (and vice versa) can still happen -- but it is less likely.

To make our join work, we need to check that we have a matching \textbf{UID} across both our datasets. We therefore need to look at the tables of both datasets and check what attributes we have that could be used for this possible match.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Open up the Attribute Tables of each layer and check what fields we have that could be used for the join. We can see that both our respective ``code'' fields have the same codes (\texttt{ward\_code} and \texttt{GSS\_code}) so we can use these to create our joins.
\item
  Right-click on your \texttt{London\_Ward} layer -\textgreater{} \textbf{Properties} and then click on the \textbf{Joins} tab.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Click on the \textbf{+} button. Make sure the \textbf{Join Layer} is set to \texttt{ward\_population\_2011}.
\item
  Set the \textbf{Join field} to \texttt{ward\_code}.
\item
  Set the \textbf{Target field} to \texttt{GSS\_code}.
\item
  Click the \textbf{Joined Fields} box and click to only select the \texttt{pop2011} field.
\item
  Click on the \textbf{Custom Field Name Prefix} and \textbf{remove} the pre-entered text to leave it blank.
\item
  Click on \textbf{OK}.
\item
  Click on \textbf{Apply} in the main Join tab and then click \textbf{OK} to return to the main QGIS window.
\end{itemize}

We can now check to see if our join has worked by opening up our \texttt{London\_Ward} \textbf{Attribute Table} and looking to see if our Wards now have a \textbf{Population} field attached to it.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Right-click on the \texttt{London\_Ward} layer and open the \textbf{Attribute Table} and check that the population data column has been added to the table.
\end{enumerate}

As long as it has joined, you can move forward with the next steps. If your join has not worked, try the steps again - and if you are still struggling, do let us know.

\textbf{Note}
Now, the join that you have created between your Ward and population datasets in only held in QGIS's memory. If you were to close the programme now, you would lose this join and have to repeat it the next time you opened QGIS. To prevent this from happening, we need to export our dataset to a new shapefile - and then re-add this to the map.

Let's do this now:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Right-click on your \texttt{London\_Ward} shapefile and click \textbf{Export} -\textgreater{} \textbf{Save Features As\ldots{}}. The format should be set to an ESRI shapefile.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Then click on the three dots buttons and navigate to your \texttt{output} folder and enter: \texttt{ward\_population\_2011} as your file name.
\item
  Check that the \textbf{CRS} is \textbf{British National Grid}.
\item
  Leave the remaining fields as selected, but check that the \textbf{Add saved file to map} is checked. Click \textbf{OK}.
\end{itemize}

You should now see our new shapefile add itself to our map. You can now remove the original \texttt{London\_Ward} and \texttt{ward\_population\_2011} datasets from our Layers box (Right-click on the layers and opt for \textbf{Remove Layer\ldots{}}).

The final thing we would like to do with this dataset is to style our dataset by our newly added population field to show population distribution around London.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  To do this, again right-click on the \textbf{Layer} -\textgreater{} \textbf{Properties} -\textgreater{} \textbf{Symbology}.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  This time, we want to style our data using a \textbf{Graduated} symbology.
\item
  Change this option in the tab and then choose \texttt{pop2011} as your column.
\item
  We can then change the color ramp to suit our aesthetic preferences - \emph{Viridis} seems to be the cool colour scheme at the moment, and we will choose to invert our ramp as well.
\item
  The final thing we need to do is \textbf{classify} our data - what this simply means is to decide how to group the values in our dataset together to create the graduated representation.
\item
  We will be looking at this in later weeks, but for now, we will use the \textbf{Natural Breaks} option.
\item
  Click on the drop-down next to Mode, select \textbf{Natural Breaks}, change it to \textbf{7} classes and then click \textbf{Classify}.
\item
  Finally click \textbf{Apply} to style your dataset.
\end{itemize}

\textbf{Note}
Understanding what classification is appropriate to visualise your data is an important step within spatial analysis and visualisation, and something you will learn more about in the following weeks. Overall, they should be determined by understanding your data's distribution and match your visualisation accordingly.

Feel free to explore using the different options with your dataset at the moment -- the results are almost instantaneous using QGIS, which makes it a good playground to see how certain parameters or settings can change your output.

You should now be looking at something like this:

\begin{figure}

{\centering \includegraphics[width=39.89in]{images/w02/ward_2011} 

}

\caption{Your result.}\label{fig:02-ward-result}
\end{figure}

You will be able to see that we have \textbf{some} missing data - and this is for several Wards within the City of London. This is because census data is \textbf{only recorded for 8 out of the 25 Wards} and therefore we have no data for the remaining Wards. As a result, these Wards are left blank, i.e.~white, to represent a \texttt{NODATA} value.

\textbf{Note}
One thing to flag is that \texttt{NODATA} means no data - whereas \texttt{0}, particularly in a scenario like this, would be an actual numeric value. It is important to remember this when processing and visualising data, to make sure you do not represent a \texttt{NODATA} value incorrectly.

\hypertarget{exporting-map-for-visual-analysis}{%
\subsection{Exporting map for visual analysis}\label{exporting-map-for-visual-analysis}}

To export your map select only the map layers you want to export and then opt for \textbf{Project} -\textgreater{} \textbf{Import/Export} -\textgreater{} \textbf{Export to Image} and save your final map in your \texttt{maps} folder. You may want to create a folder for these maps titled \texttt{w02}.

Next week, we will look at how to style our maps using the main map conventions (adding North Arrows, Scale Bars and Legends) but for now a simple picture will do.

\hypertarget{assignment-w02}{%
\section{Assignment}\label{assignment-w02}}

You now need to \textbf{repeat the entire process} for your 2015 and 2019 datasets. Remember, you need to:

\begin{itemize}
\tightlist
\item
  Load the respective Ward dataset as a Vector Layer.
\item
  Load the respective Population dataset as a Delimited Text File Layer (remember the settings!).
\item
  Join the two datasets together using the Join tool in the Ward dataset Properties box.
\item
  Export your joined dataset into a new dataset within your \texttt{output} folder.
\item
  Style your data appropriately.
\item
  Export your maps as an image to your \texttt{maps} folder.
\end{itemize}

To make visual comparisons against our three datasets, theoretically we would need to standardise the breaks at which our classification schemes are set at. To set all three datasets to the same breaks, you can do the following:

\begin{itemize}
\tightlist
\item
  Right-click on the \texttt{ward\_population\_2019} dataset and navigate to the \texttt{Symbology} tab. Double-click on the Values for the smallest classification group and set the Lower value to 141 (this is the lowest figure across our datasets, found in the 2015 data). Click \textbf{OK}, then click \textbf{Apply}, then click \textbf{OK} to return to the main QGIS screen.
\item
  Right-click again on the \texttt{ward\_population\_2019} dataset but this time, click on \textbf{Styles} -\textgreater{} \textbf{Copy Styles} -\textgreater{} \textbf{Symbology}.
\item
  Now right-click on the \texttt{ward\_population\_2015} file, but this time after clicking on \textbf{Styles} -\textgreater{} \textbf{Paste Style} -\textgreater{} \textbf{Symbology}. You should now see the classification breaks in the 2015 dataset change to match those in the 2019 data.
\item
  Repeat this for the 2011 dataset as well.
\item
  The final thing you need to do is to now change the classification column in the \texttt{Symbology} tab for the 2015 and 2011 datasets back to their original columns and press \textbf{Apply}. You will see when you first load up their Symbology options this is set to \emph{pop2019}, which of course does not exist within this dataset.
\end{itemize}

\hypertarget{byl-w02}{%
\section{Before you leave}\label{byl-w02}}

Save your project so you can go back to it if you need to, other than that \href{https://www.youtube.com/watch?v=Wmc8bQoL-J0}{that is it for this week}!

\hypertarget{cartography-and-visualisation}{%
\chapter{Cartography and Visualisation}\label{cartography-and-visualisation}}

This week's lecture has given you an in-depth introduction into how we can create a successful map, including understanding map projections, cartographic conventions and issues faced with the analysis of aggregated data at areal units. The practical component of the week puts some of these learnings into practice as we analyse crime rates within London at two different scales.

\hypertarget{reading-w03}{%
\section{Reading list}\label{reading-w03}}

\hypertarget{essential-readings-2}{%
\subsubsection*{Essential readings}\label{essential-readings-2}}
\addcontentsline{toc}{subsubsection}{Essential readings}

\begin{itemize}
\tightlist
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& Systems, \textbf{Chapter 4}: \emph{Georeferencing}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& Systems, \textbf{Chapter 11}: \emph{Cartography and Map Production}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\item
  Wong, D. 2009. Modifiable Areal Unit Problem. \emph{International Encyclopedia of Human Geography} 169-174. \href{https://doi.org/10.1016/B978-008044910-4.00475-2}{{[}Link{]}}
\end{itemize}

\hypertarget{suggested-readings-2}{%
\subsubsection*{Suggested readings}\label{suggested-readings-2}}
\addcontentsline{toc}{subsubsection}{Suggested readings}

\begin{itemize}
\tightlist
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& systems, \textbf{Chapter 12}: \emph{Geovisualization}. \href{https://ucl.rl.talis.com/link?url=https\%3A\%2F\%2Fapp.knovel.com\%2Fhotlink\%2Ftoc\%2Fid\%3AkpGISSE001\%2Fgeographic-information-science\%3Fkpromoter\%3Dmarc\&sig=e437927b963cc591dcb65491eccdd3869cc31aef80e1443cb2ba12d8f3bb031a}{{[}Link{]}}
\item
  Usery, L. and Seong, J. 2001. All equal-area map projections are created equal, but some are more equal than others. \emph{Cartography and Geographic Information Science} 28(3): 183-194. \href{https://doi.org/10.1559/152304001782153053}{{[}Link{]}}
\end{itemize}

\hypertarget{crime-in-london-i}{%
\section{Crime in London I}\label{crime-in-london-i}}

Over the next few weeks, we will look to model driving factors behind crime across London from both a statistical and spatial perspective. As \href{https://www.oxfordbibliographies.com/view/document/obo-9780195396607/obo-9780195396607-0123.xml}{Reid \emph{et al.} (2018)} argue:

\begin{quote}
Spatial analysis can be employed in both an exploratory and well as a more confirmatory manner with the primary purpose of identifying how certain community or ecological factors (such as population characteristics or the built environment) influence the spatial patterns of crime.
\end{quote}

Against this background, we are actually going to answer a very simple question today: does our perception of crime (and its distribution) in London vary at different scales? Here we are looking to test whether we would make the \textbf{ecological fallacy} mistake of assuming patterns at the Ward level are the same at the Borough level by looking to directly account for the impact of the Modifiable Area Unit Problem within our results. To test this, we will use these two administrative geographies to aggregate crime data for London in 2020. Here we will be looking specifically at a specific type of crime: \emph{Theft from a person}.

\hypertarget{w03-finding}{%
\subsection{Finding our datasets}\label{w03-finding}}

As we saw last week, accessing data within the UK, and specifically for London, is relatively straight-forward - you simply need to know which data portal contains the dataset you want!

\textbf{Note}
The datasets you will create in this practical will be used in other practicals, so make sure to follow every step and export your data into your \texttt{output} folder at the end. The practical will also introduce you to point-in-polygon counts. You will be using this type of analysis throughout this module, so it is incredibly important that you understand how they work -- even as seemingly simple as they may be!

\hypertarget{crime-data}{%
\subsubsection{Crime data}\label{crime-data}}

For our crime data, we will use data directly from the \textbf{Police Data Portal}, which you can find at \url{https://data.police.uk/}. This Data Portal allows you to access and generate tabular data for crime recorded in the U.K. across different the different Police Forces since 2017.

In total, there are 45 territorial police forces (TPF) and 3 special police forces (SPF) of the United Kingdom. Each TPF covers a specific area in the UK (e.g.~the ``West Midlands Police Force''), whilst the SPFs are cross-jurisdiction and cover specific types of crime, such as the British Transport Police. Therefore, when we want to download data for a specific area, we need to know which Police Force covers our area interest.

When you look to download crime data for London, for example, there are \textbf{two} territorial police forces working within the city and its greater metropolitan area:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{The Metropolitan Police Force (The Met)}, which covers nearly the entire London area, including Greater London
\item
  \textbf{The City of London (CoL) Police}, which covers the City of London. The Met has no jurisdiction in the CoL.
\end{enumerate}

You therefore need to decide if you want to include an analysis of crime in the City of London or not - we will in our current study.

\hypertarget{population-data}{%
\subsubsection{Population data}\label{population-data}}

We know that if we want to study a phenomena like crime and aggregate it to an areal unit, we will need to normalise this by our population. Luckily, we already sorted out our Ward population dataset last week (i.e.~\texttt{ward\_population\_2019.shp}). In addition to our \textbf{Ward} level dataset, we also want to generate the same type of shapefile for our London \textbf{Boroughs}, i.e.~a \texttt{borough\_population\_2019.shp}, utilising the same approach as last week, joining our population table data to our Borough shape data. To do this, we need to know where to get both our required datasets from - fortunately, you already have a Borough shapefile in your \texttt{raw/boundaries/2011} folder. Therefore, it is just a case of tracking down the same Mid-Year Estimates (MYE) for London Boroughs as we did for the Wards.

\textbf{Note}
Because the boundaries of the London Boroughs have not been changed since 1965, we can get away with using the 2011 \texttt{shapefile} instead of having to download a more up to date version.

\hypertarget{w03-downloading}{%
\subsection{Downloading data}\label{w03-downloading}}

As outlined above, to get going with our analysis, we need to download both the \textbf{population} data for our Boroughs and the 2020 \textbf{crime} data for our two police forces in London.

Let us tackle the population data first.

\hypertarget{borough-population}{%
\subsubsection{Borough population}\label{borough-population}}

Through a quick search, we can find our Borough population table data pretty much in the same place as our Ward data - however it is a separate spreadsheet to download.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Navigate to the data on the website of the Office for National Statistics: \href{https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/datasets/populationestimatesforukenglandandwalesscotlandandnorthernireland}{{[}Link{]}}.
\item
  Download the \textbf{Mid-2019: April 2020 local authority district codes} \texttt{.xls} file.
\item
  Open the dataset in your spreadsheet editing software and navigate to the \texttt{MYE2-Persons} tab. Now extract: \texttt{Code}, \texttt{Name}, \texttt{Geography} and \texttt{All\ ages} data for all London Boroughs.
  You should have a total of \textbf{33} Boroughs.
\item
  Once you have your 33 Boroughs separated from the rest of the data, copy the columns (\texttt{Code}, \texttt{Name}, \texttt{Geography} and \texttt{All\ ages}) and respective data for each Borough into a new \texttt{csv}.
\item
  Remember to format the \textbf{field names}. Like last week, also remember to make sure that the \texttt{All\ ages} field is recognised as a \textbf{numeric} field. Save as a new \texttt{csv} in your \texttt{output} folder: \texttt{borough\_population\_2019.csv}.
\end{enumerate}

\hypertarget{ward-population}{%
\subsubsection{Ward population}\label{ward-population}}

As mentioned above, you should already have a \texttt{ward\_population\_2019.shp} file within your \texttt{output} data folder.

\hypertarget{crime-data-1}{%
\subsubsection{Crime data}\label{crime-data-1}}

Normally, we would now head to the Police Data Portal and download our crime data. However, the manual processing that is required is too exhaustive to do manually - and far (far!) easier to do using programming. As such you can download a pre-filtered spreadsheet instead. Unzip the download and copy the \texttt{csv} into a \textbf{new} folder in your \texttt{raw} data folder called: \texttt{crime}.

\hypertarget{file-download}{%
\subsubsection*{File download}\label{file-download}}
\addcontentsline{toc}{subsubsection}{File download}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
File
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Link
\end{minipage} \\
\midrule()
\endhead
Theft from persons in 2020 & \texttt{csv} & \href{https://github.com/jtvandijk/GEOG0030/tree/master/data/zip/crime_theft_2020_london.zip}{Download} \\
\bottomrule()
\end{longtable}

\textbf{Note}
When mapping the data from the provided longitude and latitude coordinates, it is important to know that these locations represent the approximate location of a crime --- not the exact place that it happened. This displacement occurs to preserve anonymity of the individuals involved. The process by how this displacement occurs is standardised. There is a list of anonymous map points to which the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. Each map point is specifically chosen to avoid associating that point with an exact household. Interestingly enough, the police also convert the data from their recorded BNG eastings and northings into WGS84 latitude and longitude.

\hypertarget{using-qgis-to-map-our-crime-data}{%
\subsection{Using QGIS to map our crime data}\label{using-qgis-to-map-our-crime-data}}

\hypertarget{setting-up-a-project-1}{%
\subsubsection{Setting up a project}\label{setting-up-a-project-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Start \textbf{QGIS}
\item
  Click on \textbf{Project} -\textgreater{} \textbf{New}. Save your project as \texttt{w3-crime-analysis}. Remember to save your work throughout the practical.
\item
  Before we get started with adding data, we will first set the Coordinate Reference System of our Project. Click on \textbf{Project} -\textgreater{} \textbf{Properties -- CRS}. In the Filter box, type \textbf{British National Grid}. Select \textbf{OSGB 1936 / British National Grid - EPSG:27700} and click \textbf{Apply}. Click \textbf{OK}.
\end{enumerate}

Now we have our \textbf{Project CRS} set, we are ready to start loading and processing our data.

\hypertarget{adding-layers-1}{%
\subsubsection{Adding layers}\label{adding-layers-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on \textbf{Layer} -\textgreater{} \textbf{Add Layer} -\textgreater{} \textbf{Add Vector Layer}.
\item
  With \textbf{File} select as your source type, click on the small three dots button and navigate to your \texttt{ward\_population\_2019.shp} in your \texttt{output} folder. Click on the \texttt{.shp} file of this dataset and click \textbf{Open}. Then click \textbf{Add}. You may need to close the box after adding the layer.
\end{enumerate}

We now need to create our Borough population shapefile - and to do so, we need to repeat exactly the same process as last week in terms of joining our table data to our shapefile. You should now be able to do this, so we will not provide you with detailed instructions. Remember, you need to:

\begin{itemize}
\item
  Load the respective Borough dataset as a Vector Layer found in your \texttt{raw/boundaries/2011} data folder: \texttt{London\_Borough\_Excluding\_MHW.shp}.
\item
  Load the respective population dataset that you just created as a \textbf{Delimited Text File Layer}. Remember the settings, including no geometry!
\item
  Join the two datasets together using the \textbf{Join} tool in the Borough dataset \textbf{Properties} box. Remember which fields to use, which to add, and to remove the prefix.
\item
  Export your joined dataset into a new dataset within your \texttt{output} folder: \texttt{borough\_population\_2019.shp}.
\item
  Make sure this dataset is loaded into your \textbf{Layers} / Added to the map.
\item
  Remove the original Borough and population data layers.
\end{itemize}

Once this is done, we are ready to load and map our crime data. We will load this data using the \textbf{Delimited Text File Layer} option you would have used just now to load the Borough population - but this time, we will be adding point coordinates to map our crime data as points.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on \textbf{Layer} -\textgreater{} \textbf{Add Layer} -\textgreater{} \textbf{Add Delimited Text File Layer}.
\item
  With \textbf{File} select as your source type, click on the small three dots button and navigate to your \texttt{crime\_theft\_2020.csv} in your \texttt{raw/crime} folder. Click on the \texttt{.csv} file of this dataset and click \textbf{Open}. In \textbf{Record and Fields Options} tick \texttt{Decimal\ separator\ is\ comma}, \texttt{First\ record\ has\ field\ names}, \texttt{Detect\ field\ types} and \texttt{Discard\ empty\ fields}. Under \textbf{Geometry Definition}, select \texttt{Point\ coordinates} and set the \textbf{X field} to \texttt{Longitude} and the \textbf{Y field} to \texttt{Latitude}. The \textbf{Geometry CRS} should be: \texttt{EPSG:4326\ -\ WGS84}. Click \textbf{Add}.
\end{enumerate}

\textbf{Note}
You may be wondering whether we are not using the incorrect CRS for our data. As you click \textbf{Add}, you should have gotten a a pop-up from QGIS asking about transformations - they are the mathematical algorithms that convert data from one CRS to another. And this is exactly what QGIS is trying to do. QGIS knows that the \textbf{Project CRS} is \textbf{BNG} but the \textbf{Layer} you are trying to add has a \textbf{WGS84} CRS. QGIS is asking you what transformation it should use to project the Layer in the Project CRS. This is because one key strength of QGIS is that it can project data \textbf{``on the fly''}. What this means is that QGIS will automatically convert all Layers to the Project CRS once it knows which transformation you would like to use so that they will all be rendered in the correct position with respect to each other. However, you must note that this transformation is only \textbf{temporary in nature} and as a result, it is not a full \textbf{reprojection} of our data. More details on this can be found in QGIS' \href{https://docs.qgis.org/3.22/en/docs/user_manual/working_with_projections/working_with_projections.html}{user manual section on working with projections}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  For now, let us use the on-the-fly projection and accept QGIS' default value by clicking \textbf{OK}. This transformation should be built-in to your QGIS transformation library, whereas some of the more accurate options would need installation.
\end{enumerate}

You should now see your crime dataset displayed on the map.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w03/crime_unproj} 

}

\caption{Borough map with crime data.}\label{fig:03-borough-crime}
\end{figure}

We can test the `temporary' nature of the projection by looking at the CRS of the \texttt{crime\_theft\_2020} layer:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Right-click on the \texttt{crime\_theft\_2020} layer then select \textbf{Properties} -\textgreater{} \textbf{Information} and then look at the associated CRS. You should see that the CRS of the layer is still \texttt{WGS84}.
\end{enumerate}

We want to make sure our analysis is as accurate and efficient as possible, so it is best to reproject our data into the \textbf{same CRS} as our administrative datasets, i.e.~British National Grid. This also means we will have the dataset to use in other projects, just in case.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Back in the main QGIS window, click on \textbf{Vector} -\textgreater{} \textbf{Data Management Tools} -\textgreater{} \textbf{Reproject Layer}. Fill in the parameters as follows:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Input Layer:} \texttt{crime\_theft\_2020}
  \item
    \textbf{Target CRS:} \texttt{Project\ CRS:\ EPSG:\ 27700}
  \item
    \textbf{Reprojected:} Click on the three buttons and \textbf{Save to File} to create a new data file.
  \item
    \textbf{Save} it in your \texttt{output} folder as \texttt{crime\_theft\_2020\_BNG.shp}
  \item
    Click \textbf{Run} and then close the tool box.
  \end{itemize}
\end{enumerate}

You should now see the new data layer added to your Layers.

\textbf{Note}
QGIS can at times be a little bit buggy so when it creates new data layers in your Layers box, it often automates the name, hence you might see your layer added as \texttt{Reprojected}. It does this with other management and analysis tools as well, so something to be aware of.

Before moving on, let us do some housekeeping.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  Remove the original \texttt{crime\_theft\_2020} dataset, only keeping the reprojected version.
\item
  Rename the \texttt{Reprojected} dataset to \texttt{crime\_theft\_2020}.
\end{enumerate}

Now we have an organised Layers and project, we are ready to start our crime analysis.

\hypertarget{counting-points-in-polygons}{%
\subsubsection{Counting points-in-polygons}\label{counting-points-in-polygons}}

The next step of our analysis is incredibly simple - as QGIS has an in-built tool for us to use. We will use the \texttt{Count\ Points\ in\ Polygons} in the \texttt{Analysis} toolset for \texttt{Vector} data to count how many crimes have occured in both our \textbf{Wards} and our \textbf{Boroughs}. We will then have our count statistic which we will need to normalise by our population data to create our \textbf{crime rate} final statistic.

Let's get going and first start with calculating the crime rate for the Borough scale:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Click on \textbf{Vector} -\textgreater{} \textbf{Analysis Tools} -\textgreater{} \textbf{Count Points in Polygons}.
\item
  Within the toolbox, select the parameters as follows:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Polygons:} \texttt{borough\_population\_2019}
  \item
    \textbf{Points:} \texttt{crime\_theft\_2020} \emph{(Note how both our data layers state the same CRS!)}
  \item
    No weight field or class field
  \item
    \textbf{Count field names:} \texttt{crimecount}
  \item
    Click on the three dot button and \textbf{Save to file:} \texttt{output} -\textgreater{} \texttt{borough\_crime\_2020.shp}
  \end{itemize}
\item
  Click \textbf{Run} and \textbf{Close} the box. You should now see a \texttt{Count} layer added to your Layers box.
\item
  Click the checkbox next to \texttt{crime\_theft\_2020} to hide the crime points layer for now.
\item
  Right-click on the \texttt{borough\_crime\_2020} layer and open the \textbf{Attribute Table}. You should now see a \texttt{crimecount} column next to your \texttt{pop2019} column. You can look through the column to see the different levels of crime in the each Borough. You can also sort the column, from small to big, big to small, like you would do in a spreadsheet software.
\end{enumerate}

Whilst it is great that we have ve got our \texttt{crimecount}, as we know, what we actually need is a \textbf{crime rate} to account for the different population sizes accross the Boroughs. To get our \textbf{crime rate} statistic, we are going to do our first bit of table manipulation in QGIS.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  With the \textbf{Attribute Table} of your \texttt{Count} layer still open, click on the \textbf{pencil} icon at the top left corner. This pencil switches on the \textbf{Editing} mode.
\end{enumerate}

The Editing mode allows you to edit both the \textbf{Attribute Table} values and the \textbf{geometry} of your data. When it comes to the \textbf{Attribute Table}, it means you can directly edit existing values in the table \textbf{or} create and add new fields to the table. Whilst you can actually do the latter outside of the Editing mode, this Editing mode means you can reverse any edits you make and they are not permanent just in case you make a mistake.
Using the Editing mode is the safest approach to editing your table, however, it might not always be the approach you use when generating new fields.

Let us go ahead and add a new field to contain our \textbf{Crime Rate}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Whilst in the Editing mode, click on \textbf{New Field} button (or Ctrl+W/CMD+W) and fill in the \textbf{Field Parameters} as follows:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Name:} \texttt{crime\_rate}
  \item
    \textbf{Comment:} \emph{leave blank}
  \item
    \textbf{Type:} Decimal number
  \item
    \textbf{Length:} 10
  \item
    \textbf{Precision:} 0
  \end{itemize}
\item
  Click \textbf{OK},
\end{enumerate}

You should now see a new field added to our \textbf{Attribute Table}.

\textbf{Note}
Understanding how to add new fields and their parameters rely on you understanding the different data types we covered last week - and thinking through what sort of data type your field needs to contain. In our case, we will store our data as a decimal to enable our final calculation to produce a decimal (an integer/integer is likely to produce a decimal) but we will set the precision to \textbf{0} to have zero places after our decimal place when the data is used. That is because ultimately, we want our crime rate represented as an integer because, realistically, you cannot have half a crime. Calculating a decimal however will allow us to round-up within our calculations.

The empty field has \emph{NULL} populated for each row - so we need to find a way to give our Boroughs some crime rate data. To do this, we will calculate a simple \textbf{Crime Rate} using the \textbf{Field Calculator} tool provided by QGIS within the \textbf{Attribute Table}. We will create a crime rate that details the number of crimes per 10,000 people in the Borough. In most cases, a crime rate per person will create a decimal result less than 1 which not only will not be stored correctly by our \texttt{crime\_rate} field but, for many people, a \textbf{decimal} value is hard to interpret and understand.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\item
  Whilst still in the Editing mode, click on the \textbf{Abacus} button (Ctrl + I / Cmd + I), which is known as the \textbf{Field Calculator}. A new pop-up should load up.
\item
  In the Field Calculator pop-up:

  \begin{itemize}
  \tightlist
  \item
    Check the \textbf{Update existing field} box.
  \item
    Use the drop-down to select the \texttt{crime\_rate}field.
  \item
    In the Expression editor, add the following expression: \textbf{( ``crimecount'' / ``pop2019'' ) * 10000}
  \item
    You can type this in manually or use the \texttt{Fields\ and\ Values} selector in the box in the middle to add the fields into the editor.
  \item
    Once done, click \textbf{OK}.
  \end{itemize}
\end{enumerate}

You should then return to the \textbf{Attribute Table} and see our newly populated \texttt{crime\_rate} field - at the moment, we can see the resulting calculations stored as decimals.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Click on the \textbf{Save} button to save these edits. Click again on the \textbf{Pencil} button to exit Editing mode - you should see the numbers turn to integers.
\end{enumerate}

\textbf{Note}
On some computers and installations the numbers do not get changed to an \texttt{integer}, but remain a \texttt{double}. If this happens, create a new field directly in the Field Calculator. instead of ticking the \textbf{Update existing field} box you keep the \textbf{Create a new field} box ticked. Name the new field \texttt{crime\_rate\_int}, make sure the \textbf{Output field type} is set to \texttt{Whole\ number\ (integer)}, and use the same expression as above to get the crime rate in a new column.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\item
  Rename your Borough crime rate layer to \texttt{Borough\ Crime\ Rate}.
\item
  Great! We now have our Borough crime rate dataset ready for mapping and analysis - we just now need to repeat this process to have our Ward dataset. So: repeat the above processes to create a \texttt{crime\_rate} column within our Ward dataset ready for mapping and analysis.
\end{enumerate}

\hypertarget{mapping-our-crime-data}{%
\subsubsection{Mapping our crime data}\label{mapping-our-crime-data}}

Now you have both datasets ready, it is time to style the maps. Remember to use the \textbf{Properties} box to first symbolise your maps. Think through using the appropriate colour scheme - and perhaps \href{https://colorbrewer2.org/\#type=sequential\&scheme=BuGn\&n=3}{have a look online for some examples} if you do not want to use the defaults. Once you are happy with their symbolisation, we will turn them into proper publishable maps using QGIS's \textbf{Print Layout}. If you have ever used ArcMap, this is similar to switch the view of your map canvas to a print layout within the main window - but in QGIS's case, it loads up a new window.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  From the main QGIS window, click on \textbf{Project} -\textgreater{} \textbf{New Print Layout}. In the small box that first appears, call your new print layout: \texttt{crime\_map\_borough\_ward}.
\end{enumerate}

A new window should appear.

Initially, when opening the print layout provides you with a blank canvas that represents the paper surface when using the print option.

On the \textbf{left-hand} side of the window, you will find buttons beside the canvas to add print layout items: the current QGIS map canvas, text labels, images, legends, scale bars, basic shapes, arrows, attribute tables and HTML frames. In this toolbar you also find buttons to navigate, zoom in on an area and pan the view on the layout a well as buttons to select any layout item and to move the contents of the map item.

On the \textbf{right-hand} side of the window, you will find two set of panels. The upper one holds the panels Items and Undo History and the lower holds the panels Layout, Item properties and Atlas generation. For our practical today, we're most interested in the bottom panel as \textbf{Layout} will control the overall look of our map, whilst \textbf{Item properties} will allow us to customise the elements, such as Title or Legend, that we may add to our map.

In the \textbf{bottom part} of the window, you can find a status bar with mouse position, current page number, a combo box to set the zoom level and the number of selected items if applicable.

In the \textbf{upper part} of the window, you can find menus and other toolbars. All print layout tools are available in menus and as icons in a toolbar.

\textbf{Note}
Working with maps in the Print Layout is simple but it can be a little fiddly and, to make more complicated maps, requires you to understand how to use certain aspects of Print Layout, such as locking items. If you get stuck, have a look at the \href{https://docs.qgis.org/3.22/en/docs/training_manual/map_composer/map_composer.html}{training manual} or the detailed \href{https://docs.qgis.org/3.22/en/docs/user_manual/print_composer/index.html}{documentation}.

To start with creating a map, you use the \textbf{Add Map} tool to draw a box in which a snapshot of the \textbf{current active} map you have displayed in your QGIS main window will be loaded.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Click on the \textbf{Add Map} tool and draw a box in the first half of our map to load our current map. Note, you can move your map around and resize the box simply by clicking on it as you would in Word etc.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w03/map_layout} 

}

\caption{Current active map in the **Print Layout**.}\label{fig:03-map-print-layout}
\end{figure}

As you can see, the map currently does not look that great - we could really do with zooming in, as we do not need all of the white space.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  With your map selected, head to the \textbf{Items Properties} panel and look for the \textbf{Scale} parameter.

  \begin{itemize}
  \tightlist
  \item
    Here we can manually edit the \textbf{scale} of our map to find the right zoom level.
  \item
    Have a go at entering different values and see what level you think suits the size of your map.
  \item
    Keep a note of the scale, as we will need this for the second map we will add to our map layout - our Borough map.
  \item
    Next, in the same panel, if you would like, you can add a frame to your map - this will draw a box (of your selected formatting) around the current map.
  \item
    In the same panel, note down the size of your map - we want to make sure the next map we add is of the same size.
  \item
    Note, if you need to move the position of the map within the box, look for the \textbf{Move Item Content} tool on the left-hand side toolbar.
  \item
    Once you are done, finally click on the \textbf{Lock Layers} and \textbf{Lock Style for layers.}
  \end{itemize}
\end{enumerate}

By locking the Layers (and their symbology) in our map, it means we can change our data/map in our main QGIS window without changing the map in the Print Layout - as we will see in a minute when adding our Borough crime rate map. If we do not lock our layers, our map would automatically update to whatever is next displayed in the main QGIS window.

Now we have added our first map to our Map Layout, we want to add a \textbf{Legend} for this specific map.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Click on the \textbf{Add Legend} tool and again, draw a box on your map in which your legend will appear.

  \begin{itemize}
  \tightlist
  \item
    As you will see, your Legend auto-generates an entry for every layer in our Layers box in the main QGIS application:
  \end{itemize}
\end{enumerate}

\begin{center}\includegraphics[width=4.58in]{images/w03/autolegend} \end{center}

\begin{itemize}
\tightlist
\item
  In \textbf{Item Properties}, uncheck \textbf{auto-update} - this stops QGIS automatically populating your legend and enables you to customise your legend.

  \begin{itemize}
  \tightlist
  \item
    First, let us rename our Layer in the legend to: \textbf{Ward Crime Rate (per 10,000 people)}.
  \item
    Next, we want to remove all other Layers, using the \textbf{-} button
  \item
    We can also customise the Legend further, including type, size and alignment of font - go ahead and style your legend as you would prefer.
  \item
    Move the Legend to an appropriate part of the layout near your Ward crime rate map - resize if necessary.
  \end{itemize}
\end{itemize}

Now we are finished with the Ward map, we want to make sure we do not change any aspect of its layout. To do so, we need to lock both the Map and Legend in the \textbf{Items} panel - this prevents us accidentally moving items in our layout. Note, this is different to locking your layers in the \textbf{Items Properties} as we did earlier.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  In the \textbf{Items} panel, click the \textbf{Lock} check box for both our map and legend.
\end{enumerate}

Once locked, we can now start to add our Borough map.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  In the main QGIS window, uncheck your \texttt{Ward\ Crime\ Rate} layer and make sure your \texttt{Borough\ Crime\ Rate} layer is now visible.
\end{enumerate}

Return to the \textbf{Print Layout} window.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Repeat the process above of adding a map to the window - this time, you should now see your Borough map loaded in the box (and you should see \textbf{no} changes to your Ward map).

  \begin{itemize}
  \tightlist
  \item
    Place your Borough map next to your Ward map - use the snap grids to help.
  \item
    Set your \textbf{Borough} map to the \textbf{same zoom level} as your \textbf{Ward} map.
  \item
    Make sure your \textbf{Borough map} is the \textbf{same size} as your \textbf{Ward} map.
  \item
    Set your Borough map to the \textbf{same extent} as your \textbf{Ward} map (extra neatness!).
  \item
    Add a frame if you want.
  \item
    Lock your layer and its symbology in the \textbf{Items Properties} once ready and the lock your layer in the \textbf{Items} panel.
  \end{itemize}
\end{enumerate}

We now just need to add a second legend for our Borough map. If we had standardised our values across our two maps, then we would only need to use one legend. However, in this case, as there is a difference in the values, we need to have two legends.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Repeat the process as above to add a Legend for our Borough map.

  \begin{itemize}
  \tightlist
  \item
    Remember to re-title the Legend to make it more legible/informative.
  \item
    Match the same formatting for a clean look.
  \end{itemize}
\item
  Once complete, \textbf{lock these two items} in the \textbf{Items} panel as well.
\end{enumerate}

Now we have our two maps ready, we can add our \textbf{main map elements}:

\begin{itemize}
\tightlist
\item
  \textbf{Title}
\item
  \textbf{Orientation}
\item
  \textbf{Data Source}
\end{itemize}

We will not at this time add anything else - an inset map could be nice, but this requires additional data that we do not have at the moment. Any other map elements would also probably make our design look too busy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Using the tools on the left-hand tool bar:

  \begin{itemize}
  \tightlist
  \item
    Add a \textbf{scale bar}: use the \textbf{Item Properties} to adjust the \textbf{Style}, \textbf{number of segments}, \textbf{font}, etc.
  \item
    Add a \textbf{north arrow}: draw a box to generate the arrow and then use the \textbf{Item Properties} to adjust..
  \item
    Add a \textbf{title} at the top of the page, and subtitles above the individual maps.
  \item
    Finally add a box detailing our data sources, you can copy and paste the following:
    \texttt{Contains\ National\ Statistics\ data\ ©\ Crown\ copyright\ and\ database\ right\ {[}2015{]}\ (Open\ Government\ Licence).\ Contains\ Ordnance\ Survey\ data\ ©\ Crown\ copyright\ and\ database\ right\ {[}2015{]}.\ Crime\ data\ obtained\ from\ data.police.uk\ (Open\ Government\ Licence).}
  \end{itemize}
\end{enumerate}

Once you have added these properties in, you should have something that looks a little like this:

\begin{figure}

{\centering \includegraphics[width=48.71in]{images/w03/london_crime_rate} 

}

\caption{Crime rates in London boroughs and Wards. Note that the numbers in the legend may differ from the numbers in your own legend.}\label{fig:03-finale-map}
\end{figure}

The only thing outstanding is to export our map to a file. Go to \textbf{Layout} -\textgreater{} \textbf{Export as Image} and then save it in your maps folder as \texttt{London\_2020\_Crime-Rate.png}.

\hypertarget{assignment-w03}{%
\section{Assignment}\label{assignment-w03}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Looking at the maps you have created, how does your perception of crime (and its distribution) in London vary at different scales?
\end{enumerate}

At the moment, we have looked at the crime rate as an amount, therefore we use a sequential colour scheme that shows, predominantly, where the crime rate is the highest. As an alternative, we could use a diverging colour scheme that could show us where the crime rate is lower and/or higher than a critical mid-point, such as the average crime rate across the Wards or Borough.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Create a map of crime in London, at both the Borough and Ward level, that shows for each administrative geography the percentage difference from the overall mean crime rate.
\end{enumerate}

\textbf{Tips}

\begin{itemize}
\tightlist
\item
  You will need to start by calculating the average crime rate for both datasets and subsequently calculate the difference from these values for each geography.
\item
  All calculations can be done using the field calculator in QGIS, but will require some thinking about the right expression.
\end{itemize}

\hypertarget{byl-w03}{%
\section{Before you leave}\label{byl-w03}}

\href{https://www.youtube.com/watch?v=3wxyN3z9PL4}{That is us all done}. Remember to save your project and export your Ward and Borough shapefiles to your \texttt{output} folder!

\hypertarget{programming-for-data-analysis}{%
\chapter{Programming for Data Analysis}\label{programming-for-data-analysis}}

This week's content introduces you to the foundational concepts associated with Programming for Data Analysis. We will cover some general principles of programming as well how we can use R and RStudio effectively for data analysis by continuing to look at crime in London.

\hypertarget{reading-w04}{%
\section{Reading list}\label{reading-w04}}

\hypertarget{essential-readings-3}{%
\subsubsection*{Essential readings}\label{essential-readings-3}}
\addcontentsline{toc}{subsubsection}{Essential readings}

\begin{itemize}
\tightlist
\item
  Hadley, W. 2017. R for Data Science. \textbf{Chapter 4}: \emph{Workflow: basics}. \href{https://r4ds.had.co.nz/workflow-basics.html}{{[}Link{]}}
\item
  Hadley, W. 2017. R for Data Science. \textbf{Chapter 5}: \emph{Data transformation}. \href{https://r4ds.had.co.nz/transform.html}{{[}Link{]}}
\item
  Hadley, W. 2017. R for Data Science. \textbf{Chapter 6}: \emph{Workflow: scripts}. \href{https://r4ds.had.co.nz/workflow-scripts.html}{{[}Link{]}}
\item
  Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, \textbf{Chapter 1}: Introduction. \href{https://geocompr.robinlovelace.net/intro.html}{{[}Link{]}}
\end{itemize}

\hypertarget{suggested-readings-3}{%
\subsubsection*{Suggested readings}\label{suggested-readings-3}}
\addcontentsline{toc}{subsubsection}{Suggested readings}

\begin{itemize}
\tightlist
\item
  Arribas-Bel, D. \emph{et al}. 2021. Open data products - A framework for creating valuable analysis ready data. \emph{Journal of Geographical Systems} 23: 497-514. \href{https://doi.org/10.1007/s10109-021-00363-5}{{[}Link{]}}
\end{itemize}

\hypertarget{programming}{%
\section{Programming}\label{programming}}

Programming is our most fundamental way of interacting with a computer - it was how computers were first built and operated - and for a long time, the Command Line Interface (CLI) was our primary way of using computers before our Graphical User Interface (GUI) Operating Systems (OS) and software became mainstream. Nowadays, the majority of us use our computers through clicking instead of typing. However, programming and computer code underpin every single application that we use on our computers.

Programming is used for endless purposes and applications, ranging from software engineering and application development, to creating websites and managing databases at substantial scales. To help with this diversity of applications, multiple types of programming languages have developed - Wikipedia, for example, has a list of \textbf{50 different types} of languages, although there is some overlap between many of these and some are used for incredibly niche activities.

In general, the main programming languages that people focus on learning at the moment include:

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w04/programming_languages} 

}

\caption{Top 10 programming languages and their applications according to DZone in 2017.}\label{fig:04-programming-languages}
\end{figure}

\hypertarget{programming-in-r}{%
\section{Programming in R}\label{programming-in-r}}

We will be using R and RStudio in this module as the main tool to complete specific tasks we need to do for our data analysis. There are a lot of alternative tools out there that you can use to achieve the same outcomes (as you have seen with QGIS, and no doubt had experience of using some statistics/spreadsheet software) but we choose to use this tool because it provides us with many advantages over these other tools.

What is important to understand is that R and RStudio are two different things:

\begin{itemize}
\tightlist
\item
  R is our programming language, which we need to understand in terms of general principles, syntax and structure.
\item
  RStudio is our \href{https://en.wikipedia.org/wiki/Integrated_development_environment}{Integrated Development Environment (IDE)}, which we need to understand in terms of functionality and workflow. An IDE is simply a complicated way of saying ``a place where I write and build scripts and execute my code''.
\end{itemize}

As you may know already, R is a free and open-source programming language, that originally was created to focus on statistical analysis. In conjunction with the development of R as a language, the same community created the RStudio IDE to execute this statistical programming. Together, R and RStudio have grown into an incredibly success partnership of analytical programming language and analysis software - and is widely used for academic research as well as in the commercial sector. As a result, it has a huge and active contributor community which constantly adds functionality to the language and software, making it an incredibly useful tool for many purposes and applications beyond statistical analysis.

Unlike traditional statistical analysis programmes you may have used such as \href{https://www.microsoft.com/en-us/microsoft-365/excel}{Microsoft Excel} or even \href{https://www.arcgis.com/home/index.html}{ArcGIS Online}, within the RStudio IDE, the user has to type commands to get it to execute tasks such as loading in a dataset or performing a calculation. We primarily do this by building up a script, that provides a record of what you have done, whilst also enabling the straightforward repetition of tasks.

We can also use the \textbf{R Console} to execute simple instructions that do not need repeating such as installing libraries or quickly viewing data (we will get to this in a second). In addition, R, its various graphic-oriented ``packages'' and RStudio are capable of making graphs, charts and maps through just a few lines of code (you might notice a \textbf{Plots} window to your right in your RStudio window) - which can then be easily modified and tweaked by making slight changes to the script if mistakes are spotted.

Unfortunately, command-line computing can also be off-putting at first. It is easy to make mistakes that are not always obvious to detect and thus debug. Nevertheless, there are good reasons to stick with R and RStudio. These include:

\begin{itemize}
\tightlist
\item
  It is broadly intuitive with a strong focus on publishable-quality graphics.
\item
  It is `intelligent' and offers in-built good practice; it tends to stick to statistical conventions and present data in sensible ways.
\item
  It is \textbf{free}, cross-platform, customisable and extendable with a whole swathe of packages/libraries (`add ons') including those for discrete choice, multilevel and longitudinal regression, mapping, spatial statistics, spatial regression, and geostatistics.
\item
  It is well respected and used at the world's largest technology companies (including Google, Microsoft and Facebook, and at hundreds of other companies).
\item
  It offers a \textbf{transferable skill} that shows to potential employers experience both of statistics and of computing.
\end{itemize}

The intention of the practical elements of this week is to provide a thorough introduction to RStudio to get you started:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The basic programming principles behind R.
\item
  Loading in data from \texttt{csv} files, filtering and subsetting it into smaller chunks and joining them together.
\item
  Calculating a number of statistics for data exploration and checking.
\item
  Creating basic and more complex plots in order to visualise the distributions values within a dataset.
\end{enumerate}

What you should remember is that R has a steep learning curve, but the benefits of using it are well worth the effort. The best way to really learn R is to take the basic code provided in tutorials and experiment with changing parameters - such as the colour of points in a graph - to really get `under the hood' of the software.

\hypertarget{the-rstudio-interface}{%
\subsection{The RStudio interface}\label{the-rstudio-interface}}

You should all have access to some form of R on your personal computer, or through \href{https://www.ucl.ac.uk/isd/services/computers/remote-access/desktopucl-anywhere}{Desktop@UCL Anywhere} or the \href{https://rstudio.data-science.rc.ucl.ac.uk/auth-sign-in}{RStudio Server}. If not, please refer to the \protect\hyperlink{geocomputation-an-introduction.htmlux5cux23software}{Geocomputation: An Introduction} section. Go ahead and open RStudio and we will first take a quick tour of the various components of the RStudio environment interface and how and when to use them.

RStudio has various windows that you use for different purposes - and you can customise its layout dependent on your preference. When you first open RStudio, it should look a little something like this:

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w04/rstudio_interface} 

}

\caption{RStudio on RStudio Server.}\label{fig:04-rstudio-interface-script}
\end{figure}

The main windows (panel/pane) to keep focused on for now are:

\begin{itemize}
\tightlist
\item
  \textbf{Console}: where we write ``one-off'' code, such as installing libraries/packages, as well as running quick views or plots of our data.
\item
  \textbf{Files}: where our files are stored on our computer system, also helpful for general file management.
\item
  \textbf{Environment}: where our variables are recorded; we can find out \textbf{a lot} about our variables by looking at the environment window, including data structure, data type(s) and the fields and `attributes' of our variables.
\item
  \textbf{Plots}: where the outputs of our graphs, charts and maps are shown
\item
  \textbf{Help}: where you can search for help, e.g.~by typing in a function to find out its parameters.
\end{itemize}

You may also have your \textbf{Script Window} open, which is where we build up and write code, to a) keep a record of our work, b) enable us to repeat and re-run code again, often with different parameters. We will not use this window until we get to the final practical instructions.

We will see how we use these windows as we progress through this tutorial and understand in more detail what we mean by words such as `attributes' (do not get confused here with the \textbf{Attribute Table} for QGIS) and data structures.

\hypertarget{rstudio-console}{%
\section{RStudio console}\label{rstudio-console}}

We will first start off with using \textbf{RStudio's console} to test out some of R's in-built functionality by creating a few variables as well as a dummy dataset that we will be able to analyse - and to get familiar with writing code.

\textbf{Note}
You might need to click on the console window to get it to expand; you can then drag it to take up a larger space in your RStudio window.

In your RStudio console, you should see a prompt sign \texttt{\textgreater{}} on the left hand side. This is where we can directly interact with R. Anything that appears as red in the command line means it is an error (or a warning) so you will likely need to correct your code. If you just a \texttt{\textgreater{}} it means you can type in your next line, a \texttt{+} means that you have not finished the previous line of code. As will become clear, \texttt{+} signs often appear if you do not close brackets or you did not properly finish your command in a way that R expected.

In your console, let us go ahead and conduct some quick maths. At their most basic, all programming languages can be used like calculators.

\hypertarget{command-input}{%
\subsection{Command Input}\label{command-input}}

Type in \texttt{10\ *\ 12} into the console.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# conduct some maths}
\DecValTok{10} \SpecialCharTok{*} \DecValTok{12}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 120
\end{verbatim}

Once you press return, you should see the answer of 120 returned below.

\hypertarget{storing-variables}{%
\subsection{Storing variables}\label{storing-variables}}

Rather than use `raw' or `standalone' numbers and values, we primarily want to use variables that store these values (or groups of them) under a memorable name for easy reference later. In R terminology this is called \textbf{creating an object} and this object becomes stored as a \textbf{variable}. The \texttt{\textless{}-} symbol is used to assign the value to the variable name you have given. Let us create two variables for experimenting with.

Type in \texttt{ten\ \textless{}-\ 10} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store a variable}
\NormalTok{ten }\OtherTok{\textless{}{-}} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

You have just created your first variable. You will see nothing is returned in the console, but if you check your environment window it has now appeared as a new variable that contains the associated value.

Type in \texttt{twelve\ \textless{}-\ 12} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store a variable}
\NormalTok{twelve }\OtherTok{\textless{}{-}} \DecValTok{12}
\end{Highlighting}
\end{Shaded}

Once again, you will see nothing returned to the console but do check your environment window for your variable. We have now stored two numbers into our environment and given them variable names for easy reference. R stores these objects as variables in your computer's RAM so they can be processed quickly. Without saving your environment (we will come onto this below), these variables would be lost if you close R. Now we have our variables, we can go ahead and execute the same simple multiplication:

Type in \texttt{ten\ *\ twelve} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# conduct some maths  using our variables}
\NormalTok{ten }\SpecialCharTok{*}\NormalTok{ twelve}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 120
\end{verbatim}

You should see the output in the console of \textbf{120}. Whilst this maths may look trivial, it is, in fact, extremely powerful as it shows how these \textbf{variables} can be treated in the same way as the values they contain.

Next, type in \texttt{ten\ *\ twelve\ *\ 8} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# conduct some more maths with variables and raw values}
\NormalTok{ten }\SpecialCharTok{*}\NormalTok{ twelve }\SpecialCharTok{*} \DecValTok{8}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 960
\end{verbatim}

You should get an answer of 960. As you can see, we can mix \textbf{variables} with \textbf{raw values} without any problems.

We can also store the output of variable calculations as a new variable. Type \texttt{output\ \textless{}-\ ten\ *\ twelve\ *\ 8} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# conduct some maths and store it as output}
\NormalTok{output }\OtherTok{\textless{}{-}}\NormalTok{ ten }\SpecialCharTok{*}\NormalTok{ twelve }\SpecialCharTok{*} \DecValTok{8}
\end{Highlighting}
\end{Shaded}

Because we are storing the output of our maths to a new variable, the answer is not returned to the screen.

\hypertarget{accessing-and-returning-variables}{%
\subsection{Accessing and returning variables}\label{accessing-and-returning-variables}}

We can ask our computer to return this \texttt{output} by simply typing it into the console. You should see we get the same value as the earlier equation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# return the variable}
\NormalTok{output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 960
\end{verbatim}

\hypertarget{variables-of-different-data-types}{%
\subsection{Variables of different data types}\label{variables-of-different-data-types}}

We can also store variables of different data types, not just numbers but text as well.

Type in \texttt{str\_variable\ \textless{}-\ "This\ is\ our\ first\ string\ variable"} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store a variable}
\NormalTok{str\_variable }\OtherTok{\textless{}{-}} \StringTok{"This is our 1st string variable"}
\end{Highlighting}
\end{Shaded}

We have just stored our sentence made from a combination of characters, including letters and numbers. A variable that stores ``words'' (that may be sentences, or codes, or file names), is known as a string. A string is always denoted by the use of quotation marks (\texttt{""} or \texttt{\textquotesingle{}\textquotesingle{}}).

Type in \texttt{str\_variable} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# return the variable}
\NormalTok{str\_variable}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "This is our 1st string variable"
\end{verbatim}

You should see our entire sentence returned,enclosed in quotation marks (\texttt{""}). Again, by simply entering our variable into the console, we have asked R to return our variable to us.

\hypertarget{calling-functions-on-our-variables}{%
\subsection{Calling functions on our variables}\label{calling-functions-on-our-variables}}

We can also \textbf{call} a function on our variable. This use of \textbf{call} is a very specific programming term and generally what you use to say ``use'' a function. What it simply means is that we will use a specific function to do something to our variable. For example, we can also ask R to \textbf{print} our variable, which will give us the same output as accessing it directly via the console.

Type in \texttt{print(str\_variable)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# print str\_variable to the screen}
\FunctionTok{print}\NormalTok{(str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "This is our 1st string variable"
\end{verbatim}

We have just used our first function: \texttt{print()}. This function actively finds the variable and then returns this to our screen. You can type \texttt{?print} into the console to find out more about the \texttt{print()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# gain access to the documentation for our print function}
\NormalTok{?print}
\end{Highlighting}
\end{Shaded}

This can be used with any function to get access to their documentation which is essential to know how to use the function correctly and understand its output.

\textbf{Note}
In many cases, a function will take more than one argument or parameter, so it is important to know what you need to provide the function with in order for it to work. For now, we are using functions that only need one \emph{required} argument although most functions will also have several \emph{optional} or \emph{default} parameters.

\hypertarget{returning-functions}{%
\subsection{Returning functions}\label{returning-functions}}

When a function provides an output, such as this, it is known as \textbf{returning}. Not all functions will return an output to your screen, so often we require a \texttt{print()} statement or another type of returning function to check whether the function was successful or not. More on this later.

\hypertarget{examining-our-variables-using-functions}{%
\subsection{Examining our variables using functions}\label{examining-our-variables-using-functions}}

Within the base R language, there are various functions that have been written to help us examine and find out information about our variables. For example, we can use the \texttt{typeof()} function to check what data type our variable is.

Type in \texttt{typeof(str\_variable)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the typeof() function on str\_variable to return the data type of our variable}
\FunctionTok{typeof}\NormalTok{(str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

You should see the answer: \emph{character}. As evident, our \texttt{str\_variable} is a character data type. We can try testing this out on one of our earlier variables too.

Type in \texttt{typeof(ten)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the typeof() function on ten variable to return the data type of our variable}
\FunctionTok{typeof}\NormalTok{(ten)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "double"
\end{verbatim}

You should see the answer: \emph{double}. As evident, our \texttt{ten} is a double data type.

For high-level objects that involve (more complicated) data structures, such as when we load a \texttt{csv} into R as a \textbf{data frame}, we are also able to check what \textbf{class} our object is:

Type in \texttt{class(str\_variable)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the class() function on str\_variable to return the class of our object}
\FunctionTok{class}\NormalTok{(str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

In this case, you will get the same answer because in R both its class and type are the same: a \emph{character.} In other programming languages, you might have had \emph{string} returned instead, but this effectively means the same thing.

Type in \texttt{class(ten)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the class() function on ten to return the class of our object}
\FunctionTok{class}\NormalTok{(ten)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

In this case, you will get a different answer because the class of this variable is numeric. This is because the class of numeric objects can contain either doubles (decimals) or integers (whole numbers). We can test this by asking whether our \texttt{ten} variable is an integer or not.

Type in \texttt{is.integer(ten)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# test our ten variable by asking if it is an integer}
\FunctionTok{is.integer}\NormalTok{(ten)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

You should see we get the answer \texttt{FALSE}: as we know from our earlier \texttt{typeof()} function our variable ten is stored as a double and therefore cannot be an integer.

\textbf{Note}
Whilst knowing how to distinguish between different data types might not seem important now, the difference of a double versus an integer can quite easily lead to unexpected errors.

We can also ask how long our variable is. in this case, we will find out how many different sets of characters (strings) are stored in our variable, \texttt{str\_variable}.

Type in \texttt{length(str\_variable)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the length() function on str\_variable to return the length of our variable}
\FunctionTok{length}\NormalTok{(str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

You should get the answer \texttt{1} because we only have one \emph{set} of characters. We can also ask how long each set of characters is within our variable, i.e.~ask how long the string contained by our variable is.

Type in \texttt{nchar(str\_variable)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the nchar() function on str\_variable to return the length of each of our elements within our variable}
\FunctionTok{nchar}\NormalTok{(str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 31
\end{verbatim}

You should get an answer of \texttt{31}.

\hypertarget{creating-a-two-element-object}{%
\subsection{Creating a two-element object}\label{creating-a-two-element-object}}

Let us go ahead and test these two `length' functions a little further by creating a new variable to store two string sets within our object, i.e.~our variable will hold two elements.

Type in \texttt{two\_str\_variable\ \textless{}-\ c("This\ is\ our\ second\ variable",\ "It\ has\ two\ parts\ to\ it")} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store a new variable with two items using the c() function}
\NormalTok{two\_str\_variable }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"This is our second string variable"}\NormalTok{, }\StringTok{"It has two parts to it"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this piece of code, we have created a new variable using the \texttt{c()} function in R, that stands for \emph{combine values into a vector or list}. We have provided that function with two sets of strings, using a \emph{comma} to separate our two strings - all contained within the function's brackets (\texttt{()}). You should now see a new variable in your environment window which tells us it is a) chr: characters, b) contains two items, and c) lists those items.

Let us now try both our \texttt{length()} and \texttt{nchar()} on our new variable and see what the results are.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the length() function }
\FunctionTok{length}\NormalTok{(two\_str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the nchar() function}
\FunctionTok{nchar}\NormalTok{(two\_str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 34 22
\end{verbatim}

You should notice that the \texttt{length()} function now returned a \textbf{2} and the \texttt{nchar()} function returned two values of \textbf{34 and 22}.

There is one final function that we often want to use with our variables when we are first exploring them, which is \texttt{attributes()}. Because our current variables are very simple, they do not have any attributes but it is a really useful function, which we will come across later on.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# call the attributes() function }
\FunctionTok{attributes}\NormalTok{(two\_str\_variable)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## NULL
\end{verbatim}

\textbf{Note}
In addition to make notes about the functions you are coming across in the workshop, you should notice that with each line of code in the examples, an additional \textbf{comment} is used to explain what the code does. Comments are denoted using the hash symbol \texttt{\#}. This comments out that particular line so that R ignores it when the code is run. These comments will help you in future when you return to scripts a week or so after writing the code as well as help others understand what is going on when sharing your code. It is good practice to get into writing comments \textbf{as you code} and not leave it to do retrospectively. Whilst we are using the console, using comments is not necessary but as we start to build up a script later on, you will find them essential to help understand your workflow in the future.

\hypertarget{simple-analysis}{%
\section{Simple analysis}\label{simple-analysis}}

The objects we created and played with above are very simple but the real power of R comes when we can begin to execute functions on more complex objects. R accepts four main types of data structures: \textbf{vectors}, \textbf{matrices}, \textbf{data frames}, and \textbf{lists}. These data structures are essential because they allow us to apply common statistical functions.
We are going to explore these data structures with some of dummy data on the total number of pages and publication dates of the various editions of \textbf{Geographic Information Systems and Science} (GISS) book by Longley \emph{et al.} and use these for a brief analysis:

\begin{longtable}[]{@{}lcc@{}}
\toprule()
Book Edition & Year of Publication & Total Number of Pages \\
\midrule()
\endhead
1st & 2001 & 454 \\
2nd & 2005 & 517 \\
3rd & 2011 & 560 \\
4th & 2015 & 477 \\
\bottomrule()
\end{longtable}

\hypertarget{housekeeping}{%
\subsection{Housekeeping}\label{housekeeping}}

First, let us clear up our workspace and remove our current variables. Type \texttt{rm(ten,\ twelve,\ output,\ str\_variable,\ two\_str\_variable)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# clear our workspace}
\FunctionTok{rm}\NormalTok{(ten, twelve, output, str\_variable, two\_str\_variable)}
\end{Highlighting}
\end{Shaded}

You should now see we no longer have any variables in our window. We just used the \texttt{rm()} function to remove these variables from our environment and free up some RAM. Keeping a clear workspace is another recommendation of good practice moving forward. Of course, we do not want to get rid of any variables we might need to use later but removing any variables we no longer need (such as test variables) will help you understand and manage your code and your working environment.

\hypertarget{atomic-vectors}{%
\subsection{Atomic vectors}\label{atomic-vectors}}

The first complex data object we will create is a vector. A vector is the most common and basic data structure in R. Vectors are a collection of elements that are mostly of either character, logical integer or numeric data types. Technically, vectors can be one of two types:

\begin{itemize}
\tightlist
\item
  \textbf{Atomic vectors} (all elements are of the same data type)
\item
  \textbf{Lists} (elements can be of different data types)
\end{itemize}

Although in practice the term ``vector'' most commonly refers to the atomic types and not to lists. Let us create our first official ``complex'' vector, detailing the different total page numbers for GISS. Type \texttt{giss\_page\_no\ \textless{}-\ c(454,\ 517,\ 560,\ 477)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store our total number of pages, in chronological order, as a variable}
\NormalTok{giss\_page\_no }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{454}\NormalTok{, }\DecValTok{517}\NormalTok{, }\DecValTok{560}\NormalTok{, }\DecValTok{477}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Type \texttt{print(giss\_page\_no)} into the console and execute to check the results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# print our giss\_page\_no variable}
\FunctionTok{print}\NormalTok{(giss\_page\_no)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 454 517 560 477
\end{verbatim}

We can see we have our total number of pages collected together in a single vector. We could if we want, execute some statistical functions on our vector object.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate the arithmetic mean on our variable}
\FunctionTok{mean}\NormalTok{(giss\_page\_no)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 502
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate the median on our variable}
\FunctionTok{median}\NormalTok{(giss\_page\_no)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 497
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate the range numbers of our variable}
\FunctionTok{range}\NormalTok{(giss\_page\_no)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 454 560
\end{verbatim}

We have now completed our first set of descriptive statistics in R. Let us see how we can build on our vector object by adding in a second vector object that details the relevant years of our book. Note that the total number of pages are entered in a specific order to correspond to these publishing dates (i.e.~chronological) and therefore we will need to enter the publication year in the same order.

Type \texttt{giss\_year\ \textless{}-\ c(2001,\ 2005,\ 2011,\ 2015)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store our publication years, in chronological order, as a variable}
\NormalTok{giss\_year }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2001}\NormalTok{, }\DecValTok{2005}\NormalTok{, }\DecValTok{2011}\NormalTok{, }\DecValTok{2015}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Type \texttt{print(giss\_year)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# print our giss\_year variable}
\FunctionTok{print}\NormalTok{(giss\_year)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2001 2005 2011 2015
\end{verbatim}

Of course, on their own, the two vectors do not mean much but we can use the same \texttt{c()} function that we used earlier to combine the two together to create a matrix.

\hypertarget{matrices}{%
\subsection{Matrices}\label{matrices}}

In R, a matrix is simply an extension of the numeric or character vectors. They are not a separate type of object per se but simply a vector that has two dimensions. That is they contain both \textbf{rows} and \textbf{columns}. As with atomic vectors, the elements of a matrix must be of the \textbf{same data type}. As both our page numbers and our years are numeric, we can add them together to create a matrix using the \texttt{matrix()} function.

Type \texttt{giss\_year\_nos\ \textless{}-\ matrix(c(giss\_year,\ giss\_page\_no),\ ncol=2)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a new matrix from our two vectors with two columns}
\NormalTok{giss\_year\_nos }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(giss\_year, giss\_page\_no), }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{)}
\CommentTok{\# note the inclusion of a new argument to our matrix: ncol=2}
\CommentTok{\# this stands for "number of columns" and we want two}
\end{Highlighting}
\end{Shaded}

Type \texttt{print(giss\_year\_nos)} into the console and execute to check the result.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(giss\_year\_nos)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## [1,] 2001  454
## [2,] 2005  517
## [3,] 2011  560
## [4,] 2015  477
\end{verbatim}

The thing about matrices is that, for us, they do not have a huge amount of use. If we were to look at this matrix in isolation from what we know it represents, we would not really know what to do with it. As a result, we tend to primarily use \textbf{Data Frames} in R as they offer the opportunity to add \textbf{field names} to our columns to help with their interpretation.

\textbf{Note}
The function we just used above, \texttt{matrix()}, was the first function that we used that took more than one argument.
In this case, the arguments the matrix needed to run were:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What data or dataset should be stored in the matrix.
\item
  How many columns (\texttt{ncol=}) do we need to store our data in.
\end{enumerate}

For any function, there will be \textbf{mandatory} arguments (i.e.~it will not run without these) or \textbf{optional} arguments (i.e.~it will run without these, as the default to this argument has been set usually to \texttt{FALSE}, \texttt{0} or \texttt{NULL}). These are normally documented in the documentation, including details on the format the function expects these arguments to be in.

Understanding how to find out what object and data type a variable is essential therefore to knowing whether it can be used within a function or whether we will need to transform our variable into a different data structure to be used for that specific function.

\hypertarget{dataframes}{%
\subsection{Dataframes}\label{dataframes}}

A data frame is an extremely important data type in R. It is pretty much the \textbf{de-facto} data structure for most tabular data and the data structure we use for statistics. It also is the underlying structure to the table data (what we would call the \textbf{attribute table} in Q-GIS) that we associate with spatial data, more on this next week.

A data frame is a special type of list where every element of the list will have the same length (i.e.~data frame is a ``rectangular'' list), Essentially, a data frame is constructed from columns (which represent a list) and rows (which represents a corresponding element on each list). Each column will have the same amount of entries - even if, for that row, for example, the entry is simply \texttt{NULL}.

Data frames can have additional attributes such as \texttt{rownames()}, which can be useful for annotating data, like \texttt{subject\_id} or \texttt{sample\_id} or \texttt{UID}. In statistics, they are often not used but in spatial analysis, these IDs can be essential to join data together. Some additional information on data frames:

\begin{itemize}
\tightlist
\item
  They are usually created by \texttt{read.csv()} and \texttt{read.table()}, i.e.~when importing the data into R.
\item
  You can also create a new data frame with \texttt{data.frame()} function, e.g.~a matrix can be converted to a data frame.
\item
  You can find out the number of rows and columns with \texttt{nrow()} and \texttt{ncol()}, respectively.
\item
  Rownames are often automatically generated and look like X1, X2, \ldots, Xn. Consistency in numbering of rownames may not be honoured when rows are reshuffled or subset.
\end{itemize}

Let us go ahead and create a new data frame from our matrix.

Type \texttt{giss\_df\ \textless{}-\ data.frame(giss\_year\_nos)} into the console and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a new dataframe from our matrix}
\NormalTok{giss\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(giss\_year\_nos)}
\end{Highlighting}
\end{Shaded}

We now have a data frame, we can use the \texttt{View()} function in R. Still in your \textbf{console}, type: \texttt{View(giss\_df)}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# view our data frame}
\FunctionTok{View}\NormalTok{(giss\_df)}
\end{Highlighting}
\end{Shaded}

You should now see a table pop-up as a new tab on your script window. It is now starting to look like the table we are trying to create, but we need to do something about the fieldnames. \textbf{X1} and \textbf{X2} are not very informative.

\hypertarget{column-names}{%
\subsection{Column names}\label{column-names}}

We can rename our data frame column field names by using the \texttt{names()} function. Before we do this, have a read of what the \texttt{names()} function does. Still in your \textbf{console}, type: \texttt{?names}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get the help documentation for the names function}
\NormalTok{?names}
\end{Highlighting}
\end{Shaded}

As you can see, the function will get or set the names of an object, with renaming occurring by using the following syntax: \texttt{names(x)\ \textless{}-\ value}

The value itself needs to be \emph{a character vector of up to the same length as x, or NULL.} We have two columns in our data frame, so we need to parse our \texttt{names()} function with a character vector with \textbf{two elements}. In the console, we shall enter two lines of code, one after another. First our character vector with our new names, \texttt{new\_names\ \textless{}-\ c("year",\ "page\_nos")}, and then the \texttt{names()} function containing this vector for renaming, \texttt{names(giss\_df)\ \textless{}-\ new\_names}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a vector with our new column names}
\NormalTok{new\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"page\_nos"}\NormalTok{)}

\CommentTok{\# rename our columns with our next names}
\FunctionTok{names}\NormalTok{(giss\_df) }\OtherTok{\textless{}{-}}\NormalTok{ new\_names}
\end{Highlighting}
\end{Shaded}

You can go and check your data frame again and see the new names using either \texttt{View()} function or by clicking on the tab at the top.

\hypertarget{adding-columns}{%
\subsection{Adding columns}\label{adding-columns}}

We are still missing one final column from our data frame: our \textbf{edition} of the textbook column. As this is a \texttt{character} data type, we would not have been able to add this directly to our matrix. This is because data frames can take different data types, unlike matrices - so let us go ahead and add the \textbf{edition} as a new column.

To do so, we follow a similar process of creating a vector with our editions listed in chronological order, but then add this to our data frame by storing this vector as a new column in our data frame. We use the \texttt{\$} sign with our code that gives us ``access'' to the data frame's column - we then specify the column \texttt{edition}, which whilst it does not exist at the moment, will be created from our code that assigns our edition variable to this column.

Type and execute \texttt{edition\ \textless{}-\ c("1st",\ "2nd",\ "3rd",\ "4th")}. Then store this vector as a new column in our data frame under the column name \textbf{edition} by typing and executing \texttt{giss\_df\$edition\ \textless{}-\ edition}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a vector with our editions}
\NormalTok{edition }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"1st"}\NormalTok{, }\StringTok{"2nd"}\NormalTok{, }\StringTok{"3rd"}\NormalTok{, }\StringTok{"4th"}\NormalTok{)}

\CommentTok{\# add this vector as a new column to our data frame}
\NormalTok{giss\_df}\SpecialCharTok{$}\NormalTok{edition }\OtherTok{\textless{}{-}}\NormalTok{ edition}
\end{Highlighting}
\end{Shaded}

Again, you can go and check your data frame and see the new column using either \texttt{View()} function or by clicking on the tab at the top or by typing \texttt{giss\_df} in your console window.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# inspect}
\NormalTok{giss\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   year page_nos edition
## 1 2001      454     1st
## 2 2005      517     2nd
## 3 2011      560     3rd
## 4 2015      477     4th
\end{verbatim}

Now we have our data frame, let us find out a little about it. We can first return the dimensions (the size) of our data frame by using the \texttt{dim()} function. In your \textbf{console}, type \texttt{dim(giss\_df)} and execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check our data frame dimensions}
\FunctionTok{dim}\NormalTok{(giss\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 3
\end{verbatim}

We can see we have \textbf{four rows} and \textbf{three columns}. We can also finally use our \texttt{attributes()} function to get the attributes of our data frame. In your \textbf{console}, type \texttt{attributes(giss\_df)} and execute:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check our data frame attributes}
\FunctionTok{attributes}\NormalTok{(giss\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $names
## [1] "year"     "page_nos" "edition" 
## 
## $row.names
## [1] 1 2 3 4
## 
## $class
## [1] "data.frame"
\end{verbatim}

\textbf{Tips}

\begin{itemize}
\tightlist
\item
  R is case-sensitive so you need to make sure that you capitalise everything correctly if required.
\item
  The spaces between the words do not matter but the positions of the commas and brackets do. Remember, if you find the prompt, \texttt{\textgreater{}}, is replaced with a \texttt{+} it is because the command is incomplete. If necessary, hit the escape (\texttt{esc}) key and try again.
\item
  It is important to come up with good names for your objects. In the case of the majority of our variables, we used a underscore \texttt{\_} to separate the words. It is good practice to keep the object names as short as possible but they still need to be easy to read and clear what they are referring to. Be aware: \textbf{you cannot start an object name with a number!}
\item
  If you press the up arrow in the command line you will be able to edit the previous lines of code you have inputted.
\end{itemize}

\hypertarget{crime-in-london-ii}{%
\section{Crime in London II}\label{crime-in-london-ii}}

During \protect\hyperlink{geocomputation-an-introduction.htmlux5cux23r-package-installation}{Week 1's computer tutorial}, we already installed several R libraries. One of these libraries was called the \texttt{tidyverse}. The \texttt{tidyverse} is a collection of packages that are specifically designed for data wrangling, management, cleaning, analysis and visualisation within RStudio. Whilst in many cases different packages work all slightly differently, all packages of the \texttt{tidyverse} share the underlying design philosophy, grammar, and data structures.

The tidyverse itself is treated and loaded as a single package, but this means if you load the \texttt{tidyverse} package within your script (through \texttt{library(tidyverse)}), you will directly have access to all the functions that are part of each of the packages that are within the overall \texttt{tidyverse.} This means you do not have to load each package separately. For more information have a look at \url{https://www.tidyverse.org/}.

There are some specific functions in the \texttt{tidyverse} suite of packages that will help us cleaning and preparing our datasets now and in the future, which is one of the main reasons for using this library. Some of the most important and useful functions, from the \texttt{tidyr} and \texttt{dplyr} packages, are:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Package
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Use to
\end{minipage} \\
\midrule()
\endhead
dplyr & select & select columns \\
dplyr & filter & select rows \\
dplyr & mutate & transform or recode variables \\
dplyr & summarise & summarise data \\
dplyr & group by & group data into subgroups for further processing \\
tidyr & pivot\_longer & convert data from wide format to long format \\
tidyr & pivot\_wider & convert long format dataset to wide format \\
\bottomrule()
\end{longtable}

These functions all complete very fundamental tasks that we need to manipulate and wrangle our data.

\textbf{Note}
The code you just ran asked \textbf{R} to load all functions of the \texttt{tidyverse}. However: these functions are only available for the duration of your R sessions. When you restart your R session, you will have to load these functions again if you want to use them. Another thing to be aware of when it comes to using functions in these additional libraries, is that sometimes these functions share a name with a function form one of the \texttt{base} \textbf{R} packages. For instance, there exists a \texttt{select()} function within in the \texttt{stats} package that conducts linear filtering on a time series. However, after we load the \texttt{tidyverse} package and we would type \texttt{select()} this function will select columns from a data frame. We therefore sometimes need to specify which library we want to use this function from, and this can be done with a simple command (\texttt{library::function}) in our code: \texttt{stats::select} to filter that time series and \texttt{dplyr::select} to select columns in a dataframe

\hypertarget{starting-a-project}{%
\subsection{Starting a project}\label{starting-a-project}}

In the previous section, R may have seemed fairly labour-intensive. We had to enter all our data manually and each line of code had to be written into the command line. Fortunately this is not routinely the case. In RStudio, we can use \textbf{scripts} to build up our code that we can run repeatedly and save for future use. Before we start a new script, we first want to set up ourselves ready for the rest of our practicals by \textbf{creating a new project}.

To put it succinctly, \textbf{projects} in RStudio keep all the files associated with a project together: input data, R scripts, analytical results, figures, etc.. This means we can easily keep track of all data, input and output, whilst still creating standalone scripts for each bit of processing analysis we do. It also makes dealing with \textbf{directories} and \textbf{paths} a whole lot easier; particularly if you \textbf{have followed the folder structure that was advised at the start of the module}.

Click on \textbf{File} -\textgreater{} \textbf{New Project} -\textgreater{} \textbf{Existing Directory} and browse to your \textbf{GEOG0030} folder. Click on \textbf{Create Project}. You should now see your main window switch to this new project and if you check your \textbf{Files} window, you should now see a new \textbf{R Project} called \textbf{GEOG0030}.

\textbf{Note}
Please ensure that \textbf{folder names} and \textbf{file names} do not contain spaces or special characters such as \texttt{*} \texttt{.} \texttt{"} \texttt{/} \texttt{\textbackslash{}} \texttt{{[}} \texttt{{]}} \texttt{:} \texttt{;} \texttt{\textbar{}} \texttt{=} \texttt{,} \texttt{\textless{}} \texttt{?} \texttt{\textgreater{}} \texttt{\&} \texttt{\$} \texttt{\#} \texttt{!} \texttt{\textquotesingle{}} \texttt{\{} \texttt{\}} \texttt{(} \texttt{)}. Different operating systems and programming languages deal differently with spaces and special characters and as such including these in your folder names and file names can cause many problems and unexpected errors. As an alternative to using white space you can use an underscore \texttt{\_} if you like.

\hypertarget{setting-up-a-script}{%
\subsection{Setting up a script}\label{setting-up-a-script}}

For the majority of our analysis work, we will type our code \textbf{within a script} and not the console. Let us create our first script. Click on \textbf{File} -\textgreater{} \textbf{New File} -\textgreater{} \textbf{R Script}. This should give you a blank document that looks a bit like the command line. The difference is that anything you type here can be saved as a script and re-run at a later date.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w04/new-script} 

}

\caption{Creating a new script.}\label{fig:04-rstudio-interface-script-open}
\end{figure}

Save your script as: \texttt{wk4-csv-processing.r}. Through our name, we know now that our script was created in Week 4 of Geocomputation and the code it will contain is something to do with \texttt{csv} processing. This will help us a lot in the future when we come to find code that we need for other projects.

The \textbf{first} bit of code you will want to add to any script is to add a \textbf{title}. This title should give any reader a quick understanding of what your code achieves. When writing a script it is important to keep notes about what each step is doing. To do this, the hash (\texttt{\#}) symbol is put before any code. This comments out that particular line so that R ignores it when the script is run.

Let us go ahead and give our script a title - and maybe some additional information:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Combining Police Data csv\textquotesingle{}s from 2020 into a single csv}
\CommentTok{\# Followed by analysis of data on monthly basis}
\CommentTok{\# Date: January 2023}
\CommentTok{\# Author: Justin }
\end{Highlighting}
\end{Shaded}

Now we have our title, the \textbf{second} bit of code we want to include in our script is to load \textbf{our libraries} (i.e.~the installed packages we want to use in our script):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

By loading simply the \texttt{tidyverse} we gain access to several useful functions. However, when developing a script you will realise that you may need to load more libraries as you go along. When you do this, \textbf{always add your library to the top of your script}. If you ever share your script, it helps the person you are sharing with to recognise quickly if they need to install any additional packages prior to running the code It also means your libraries do not get lost in the multiple lines of code you are writing.

We are now ready to run these first two lines of code. Remember to save your script.

\hypertarget{running-a-script}{%
\subsection{Running a script}\label{running-a-script}}

There are two main ways to run a script in RStudio: all at once or by line/chunk. It can be advantageous to pursue with the second option as you first start out to build your script as it allows you to test your code interactively.

\hypertarget{to-run-line-by-line}{%
\subsubsection*{To run line-by-line}\label{to-run-line-by-line}}
\addcontentsline{toc}{subsubsection}{To run line-by-line}

By clicking:

\begin{itemize}
\tightlist
\item
  Select the line or chunk of code you want to run, then click on \textbf{Code} and choose \textbf{Run selected lines}.
\end{itemize}

By key commands:

\begin{itemize}
\tightlist
\item
  Select the line or chunk of code you want to run and then hold \textbf{Ctl} or \textbf{Cmd} and press \textbf{Return}.
\end{itemize}

\hypertarget{to-run-the-whole-script}{%
\subsubsection*{To run the whole script}\label{to-run-the-whole-script}}
\addcontentsline{toc}{subsubsection}{To run the whole script}

By clicking:

\begin{itemize}
\tightlist
\item
  Click on \textbf{Run} on the top-right of the scripting window and choose \textbf{Run All}.
\end{itemize}

By key commands:

\begin{itemize}
\tightlist
\item
  Hold \textbf{Option} plus \textbf{Ctl} or \textbf{Cmd} and \textbf{R}.
\end{itemize}

\hypertarget{stopping-a-script-from-running}{%
\subsubsection*{Stopping a script from running}\label{stopping-a-script-from-running}}
\addcontentsline{toc}{subsubsection}{Stopping a script from running}

If you are running a script that seems to be stuck (for whatever reason) or you notice some of your code is wrong, you will need to interrupt R. To do so, click on \textbf{Session} -\textgreater{} \textbf{Interrupt R}. If this does not work, you may end up needing to \textbf{Terminate R} but this may lose any unsaved progress.

\hypertarget{crime-data-2}{%
\subsection{Crime data}\label{crime-data-2}}

Where last week we provided you with a crime dataset, this week you will download and prepare the dataset yourself.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start by navigating to \href{https://data.police.uk/}{data.police.uk}. And click on \textbf{Downloads}.
\item
  Under the data range select \texttt{January\ 2020} to \texttt{December\ 2020}.
\item
  Under the \textbf{Custom download} tab select \texttt{Metropolitan\ Police\ Service} and \texttt{City\ of\ London\ Police}. Leave all other settings and click on \textbf{Generate file}.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w04/download-crime} 

}

\caption{Downloading our crime data.}\label{fig:04-download-crimes}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  It may take a few minutes for the download to be generated, so be patient. Once the \textbf{Download now} button appears, you can download the 2020 crime dataset.
\item
  Once downloaded, unzip the file. You will notice that the zip file contains 12 individual folders, one for each month in 2020. Each folder contains two files: one containing the data for the \texttt{Metropolitan\ Police\ Service} and one for the \texttt{City\ of\ London\ Police}.
\item
  Create a new folder named \texttt{all\_crime} in your \texttt{data/raw/crime} directory and copy all 12 folders containing our data to this new folder.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=14.33in]{images/w04/organised-crime} 

}

\caption{Your data folder should now look something like this.}\label{fig:04-organised-crime}
\end{figure}

\hypertarget{reading-data-into-r}{%
\subsubsection{Reading data into R}\label{reading-data-into-r}}

We are now ready to get started with using the crime data \texttt{csv\textquotesingle{}s} currently sat in our \texttt{all\_crime} folder. To do so, we need to first figure out how to import the \texttt{csv} and understand the data structure it will be in after importing. To read in a \texttt{csv} into R requires the use of a very simple function from the \texttt{tidyverse} library: \texttt{read\_csv()}.

We can look at the help documentation to understand what we need to provide the function (or rather the optional arguments), but as we just want to load single \texttt{csv}, we will go ahead and just use the function with a simple parameter.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in a single csv from our crime data}
\NormalTok{crime\_csv }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/raw/crime/all\_crime/2020{-}01/2020{-}01{-}metropolitan{-}street.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Note}
If using a Windows machine, you will need to substitute your forward-slashes (\texttt{/}) with two backslashes (\texttt{\textbackslash{}\textbackslash{}}) whenever you are dealing with file paths!

We can explore the \texttt{csv} we have just loaded as our new \texttt{crime\_csv} variable and understand the class, attributes and dimensions of our variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check class }
\FunctionTok{class}\NormalTok{(crime\_csv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "spec_tbl_df" "tbl_df"      "tbl"         "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check dimensions}
\FunctionTok{dim}\NormalTok{(crime\_csv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 90979    12
\end{verbatim}

We have found out our variable is a data frame, containing \textbf{90,979 rows and 12 columns}. We however do not want just the single \texttt{csv} and instead what to combine all our \texttt{csv\textquotesingle{}s} in our \texttt{all\_crime} folder into a single data frame - so how do we do this?

This will be the most complicated section of code you will come across today, and we will use some functions that you have not seen before. Copy the following code below into your script, then execute.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in all files and append all rows to a single data frame}
\NormalTok{all\_crime\_df }\OtherTok{\textless{}{-}} \FunctionTok{list.files}\NormalTok{(}\AttributeTok{path=}\StringTok{"data/raw/crime/all\_crime/"}\NormalTok{, }\AttributeTok{full.names=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{recursive=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{lapply}\NormalTok{(read\_csv) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  bind\_rows}
\end{Highlighting}
\end{Shaded}

This might take a little time to process (or might not), as we have a lot of data to get through. You should see a new data frame appear in your global environment called \texttt{all\_crime\_df}, for which we now have 1,187,847 observations!

\textbf{Note}
It is a little difficult to explain the code above without going into too much detail and at this stage you are not expected to fully understand what is happening here, but essentially what the code does is:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  List all the files found in the data path: \texttt{data/raw/crime/all\_crime/}
\item
  Read each of these as a \texttt{csv} by ``applying'' the \texttt{read\_csv()} function to all files.
\item
  Binding all rows of all individual data frames together in a single data frame.
\end{enumerate}

These three different actions are combined by using something called a pipe (\texttt{\%\textgreater{}\%} or \texttt{\textbar{}\textgreater{}}), which we will explain in more detail in later weeks.

\hypertarget{inspecting-data-in-r}{%
\subsubsection{Inspecting data in R}\label{inspecting-data-in-r}}

We can now have a look at our large dataframe in more detail.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# understand our all\_crime\_df cols, rows and print the first five rows}
\FunctionTok{ncol}\NormalTok{(all\_crime\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(all\_crime\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1187847
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(all\_crime\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 12
##   Crime ~1 Month Repor~2 Falls~3 Longi~4 Latit~5 Locat~6 LSOA ~7 LSOA ~8 Crime~9
##   <chr>    <chr> <chr>   <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  
## 1 37c663d~ 2020~ City o~ City o~ -0.106     51.5 On or ~ E01000~ Camden~ Theft ~
## 2 5b89923~ 2020~ City o~ City o~ -0.118     51.5 On or ~ E01000~ Camden~ Drugs  
## 3 0717268~ 2020~ City o~ City o~ -0.112     51.5 On or ~ E01000~ Camden~ Other ~
## 4 14e02a6~ 2020~ City o~ City o~ -0.111     51.5 On or ~ E01000~ Camden~ Other ~
## 5 fb3350c~ 2020~ City o~ City o~ -0.113     51.5 On or ~ E01000~ Camden~ Other ~
## 6 <NA>     2020~ City o~ City o~ -0.0976    51.5 On or ~ E01000~ City o~ Anti-s~
## # ... with 2 more variables: `Last outcome category` <chr>, Context <lgl>, and
## #   abbreviated variable names 1: `Crime ID`, 2: `Reported by`,
## #   3: `Falls within`, 4: Longitude, 5: Latitude, 6: Location, 7: `LSOA code`,
## #   8: `LSOA name`, 9: `Crime type`
\end{verbatim}

You should now see with have the same number of columns as our previous single \texttt{csv}, but with many more rows. You can also see that the \texttt{head()} function provides us with the first \textbf{five} rows of our data frame. You can conversely use \texttt{tail()} to provide the last five rows.

For now in our analysis, we only want to extract the \textbf{theft} crime in our data frame - so we will want to filter our data based on the \textbf{Crime type} column. However, as we can see, we have a space in our field name for \textbf{Crime type} and, in fact, many of the other fields. As we want to avoid having spaces in our field names when coding, we need to rename our fields. Rename the field names, just as we did with our GIS table earlier:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a new vector containing updated no space / no capital field names}
\NormalTok{no\_space\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"crime\_id"}\NormalTok{, }\StringTok{"month"}\NormalTok{, }\StringTok{"reported\_by"}\NormalTok{, }\StringTok{"falls\_within"}\NormalTok{, }\StringTok{"longitude"}\NormalTok{,}\StringTok{"latitude"}\NormalTok{, }\StringTok{"location"}\NormalTok{, }\StringTok{"lsoa\_code"}\NormalTok{, }\StringTok{"lsoa\_name"}\NormalTok{, }\StringTok{"crime\_type"}\NormalTok{, }\StringTok{"last\_outcome\_category"}\NormalTok{, }\StringTok{"context"}\NormalTok{)}

\CommentTok{\# rename our df field names using these new names}
\FunctionTok{names}\NormalTok{(all\_crime\_df) }\OtherTok{\textless{}{-}}\NormalTok{ no\_space\_names}
\end{Highlighting}
\end{Shaded}

We now have our dataframe ready for filtering. To do so, we will use the \texttt{filter()} function from the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter all\_crime\_df to contain only theft, store as a new variable: all\_theft\_df}
\NormalTok{all\_theft\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(all\_crime\_df, crime\_type }\SpecialCharTok{==} \StringTok{"Theft from the person"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You should now see the new variable appear in your environment with \textbf{31,578} observations.

\textbf{Note}
Although not necessary per se because R uses the function from the package that got loaded latest, it is a good idea to specify that we want to use \texttt{filter()} from the \texttt{dplyr} package instead of the default \texttt{stats} library so to avoid any confusing errors.

We now want to do some further housekeeping and create on final data frame that will allow us to analyse crime in London by month. To do so, we want to count how many thefts occur each month in London. Fortunately, \texttt{dplyr} has another function that will do this for us, known simply as \texttt{count()}.

When you go ahead and search the documentation to understand the \texttt{count()} function, you will see that there is only one function called \texttt{count()} at the moment, i.e.~the one in the \texttt{dplyr} library, so we do not need to use the additional syntax we used above. Let us go ahead and count the number of thefts in London \textbf{by month}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# count in the all\_theft\_df the number of crimes by month and store as a new dataframe}
\NormalTok{theft\_month\_df }\OtherTok{\textless{}{-}} \FunctionTok{count}\NormalTok{(all\_theft\_df, month)}
\end{Highlighting}
\end{Shaded}

We have stored the output of our \texttt{count()} function to a new data frame: \texttt{theft\_month\_df}. Go ahead and look at the data frame to see the output: it is a very simple table containing simply the month and \texttt{n}, i.e.~the number of thefts occurring per month. We can and should go ahead and rename this column to help with our interpretation of the data frame. We will use a quick approach to do this, that uses \textbf{selection} of the precise column to rename only the second column:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# rename the second column of our new data frame to crime\_totals}
\FunctionTok{names}\NormalTok{(theft\_month\_df)[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"crime\_totals"}
\end{Highlighting}
\end{Shaded}

This selection is made through the \texttt{{[}2{]}} element of code added after the \texttt{names()} function we have used earlier. We will look more at selection, slicing and indexing during next week's tutorial.

\hypertarget{assignment-w04}{%
\section{Assignment}\label{assignment-w04}}

Now we have prepared our dataset, we can conduct some analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What was the average number of crimes per month in London in 2020?
\item
  What was the median number of crimes per month in London in 2020?
\item
  What were the minimum and maximum values of crime in London in 2020?
\item
  Besides descriptive statistics, it would be really useful to generate a simple chart. Use the documentation of the \texttt{barplot()} function to create the barplot below:
\end{enumerate}

\includegraphics{GEOG0030_files/figure-latex/04-bar-plot-1.pdf}

\textbf{Note}
Do not forget to save your script so you can go back to it at a later time. When you close R and are asked if you want to save your workspace: this is not necessary. Saving the workspace will keep any variables generated during your current session saved and available in a future session, but so will re-running your script.

\hypertarget{want-more-optional}{%
\section{Want more? {[}Optional{]}}\label{want-more-optional}}

The graphs and figures we have made so far are rather basic. Although possible with the basic R installation, there are easier and better ways to make nice visualisations. For this we can turn to other R packages that have been developed. In fact, there are many hundreds of packages in R each designed for a specific purpose, some of which you can use to create plots in R. One of those packages is called \texttt{ggplot2}. The \texttt{ggplot2} package is an implementation of the Grammar of Graphics (\href{https://en.wikipedia.org/wiki/Leland_Wilkinson}{Wilkinson 2005}) - a general scheme for data visualisation that breaks up graphs into semantic components such as scales and layers. \texttt{ggplot2} can serve as a replacement for the base graphics in R and contains a number of default options that match good visualisation practice. You provide the data, tell \texttt{ggplot2} how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. An excellent introduction to \texttt{ggplot2} can be found in the online, freely available book \href{https://r4ds.had.co.nz/data-visualisation.html}{R for Data Science}; written by Hadley Wickham, core developer of \texttt{ggplot2} and the \texttt{tidyverse}. Have a particularly close look at \textbf{Chapter 3}: \emph{Data visualisation}: \href{https://r4ds.had.co.nz/data-visualisation.html}{{[}Link{]}}.

\hypertarget{byl-w04}{%
\section{Before you leave}\label{byl-w04}}

We have managed to take a dataset of over one million records and clean and filter it to create a barplot on theft crime in London. Of course, there is a lot more research and exploratory data analysis that can be done, but this first chart is certainly a step in the right direction. Next week, we will be doing a lot more with our dataset - including a lot more data wrangling and of course spatial analysis, but hopefully this week has shown you want you can achieve with just a few lines of code. \href{https://www.youtube.com/watch?v=NKyYr2pbXUM}{That concludes the tutorial for this week}!

\hypertarget{programming-for-spatial-analysis}{%
\chapter{Programming for Spatial Analysis}\label{programming-for-spatial-analysis}}

This week we are going to look at how to use R and RStudio as a piece of GIS software. Like last week, we will be completing an analysis on our London theft crime data set. However, rather than solely looking at changing crime rates over time, we will add a spatial component to our analysis: how has crime changed across our wards over the years.

\hypertarget{reading-w05}{%
\section{Reading list}\label{reading-w05}}

\hypertarget{essential-readings-4}{%
\subsubsection*{Essential readings}\label{essential-readings-4}}
\addcontentsline{toc}{subsubsection}{Essential readings}

\begin{itemize}
\tightlist
\item
  Longley, P. \emph{et al.} 2015. Geographic Information Science \& systems, \textbf{Chapter 13}: \emph{Spatial Analysis}. \href{https://rl.talis.com/3/ucl/items/fd38ec78-2bea-4165-aab3-0e9d9093db8e.html?lang=en-gb\&login=1}{{[}Link{]}}
\item
  Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, \textbf{Chapter 2}: \emph{Geographic Data in R}. \href{https://geocompr.robinlovelace.net/spatial-class.html}{{[}Link{]}}
\item
  Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, \textbf{Chapter 3}: \emph{Attribute data operations}. \href{https://geocompr.robinlovelace.net/geometric-operations.html}{{[}Link{]}}
\item
  Lovelace, R., Nowosad, J. and Muenchow, J. 2021. Geocomputation with R, \textbf{Chapter 8}: \emph{Making maps with R}. \href{https://geocompr.robinlovelace.net/adv-map.html}{{[}Link{]}}
\end{itemize}

\hypertarget{suggested-readings-4}{%
\subsubsection*{Suggested readings}\label{suggested-readings-4}}
\addcontentsline{toc}{subsubsection}{Suggested readings}

\begin{itemize}
\tightlist
\item
  Poorthuis, A. and Zook, M. 2020. Being smarter about space: Drawing lessons from spatial science. \emph{Annals of the American Association of Geographers} 110(2): 349-359. \href{https://doi.org/10.1080/24694452.2019.1674630}{{[}Link{]}}
\item
  De Smith, M, Goodchild, M. and Longley, P. 2018. Geospatial analsyis. A Comprehensive guide to principles techniques and software tools. \textbf{Chapter 9}: \emph{Big Data and geospatial analysis}. \href{https://arxiv.org/pdf/1902.06672.pdf}{{[}Link{]}}
\item
  Radil, S. 2016. Spatial analysis of crime. \emph{The Handbook of Measurement Issues in Criminology and Criminal Justice} 536-554. \href{https://doi.org/10.1002/9781118868799.ch24}{{[}Link{]}}
\end{itemize}

\hypertarget{crime-in-london-iii}{%
\section{Crime in London III}\label{crime-in-london-iii}}

To analyse crime by ward over time, we will go through several steps of data preparation (`data wrangling') before joining our data to ward polygons. We will start by taking our \texttt{all\_theft\_df} dataframe and wrangle it to produce a dataframe with a for each ward the number of crimes for each month of data. we then join this dataframe to our \texttt{ward\_population\_2019} shapefile (which should still be sitting in your \texttt{output} folder) and then produce a \textbf{crime rate} for each month, for each ward. Lastly, we will create a map of the crime rate in London for January 2020 using the \texttt{tmap} library.

\hypertarget{data-preparation}{%
\subsection{Data preparation}\label{data-preparation}}

Before we get started, we first need to head back to our script from last week, run our script - and then write our \texttt{all\_theft\_df} to a \texttt{csv} file. If you had saved your environment from last week, keeping your variables in memory, theoretically you would not need to export the data frame as you should have access to this variable within your new script but it would be good practice to write out the data and then load it back in.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Open up your \textbf{GEOG0030} RStudio project.
\item
  Next open your script from last week: \texttt{wk4-csv-processing.r}.
\item
  Run your full script up to and \textbf{including} the code that \textbf{filters} our large \texttt{all\_crime\_df} to only the \texttt{all\_theft\_df}.
\item
  Scroll to the bottom of the script and enter the following code and execute:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write out the theft\_crime\_df to a csv within our raw crime data folder}
\FunctionTok{write.csv}\NormalTok{(all\_theft\_df,}\StringTok{"data/raw/crime/crime\_theft\_2020\_london.csv"}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Note}
If using a Windows machine, you will need to substitute your forward-slashes (\texttt{/}) with two backslashes (\texttt{\textbackslash{}\textbackslash{}}) whenever you are dealing with file paths!

You should now see a new \texttt{csv} within your raw crime data folder (\texttt{data/raw/crime}). You can now save your \texttt{wk4-csv-processing.r} script and close the script.

\hypertarget{spatial-analysis-set-up}{%
\subsection{Spatial analysis set up}\label{spatial-analysis-set-up}}

Open a new script within your GEOG0030 project and save this script as \texttt{wk5-crime-spatial-processing.r}. At the top of your script, add the following metadata (substitute accordingly):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Analysing crime in 2020 by month and ward}
\CommentTok{\# Date: January 2023}
\CommentTok{\# Author: Justin }
\end{Highlighting}
\end{Shaded}

Now let us add \textbf{all} of the libraries we will be using today to the top of our script:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# libraries}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(tmap)}
\end{Highlighting}
\end{Shaded}

You are already familiar with the \texttt{tidyverse} library, but now we adding \texttt{sf} to read and load our spatial data as well as \texttt{tmap} to visualise our spatial data. We are going to load both of the data sets we need today straight away: the \texttt{crime\_theft\_2020\_london.csv} we have just exported and the \texttt{ward\_population\_2019.shp} we created in Week 3.

First, let's load our \texttt{crime\_theft\_2020\_london.csv} into a dataframe called \texttt{all\_theft\_df}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in our crime\_theft\_2020\_london csv from our raw crime data folder}
\NormalTok{all\_theft\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/raw/crime/crime\_theft\_2020\_london.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can double-check what our \texttt{csv} looks like by either viewing our data or simply calling the \texttt{head()} function on our dataframe:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check the first five rows of our all\_theft dataframe}
\FunctionTok{head}\NormalTok{(all\_theft\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 12
##   crime_id month repor~1 falls~2 longi~3 latit~4 locat~5 lsoa_~6 lsoa_~7 crime~8
##   <chr>    <chr> <chr>   <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  
## 1 37c663d~ 2020~ City o~ City o~ -0.106     51.5 On or ~ E01000~ Camden~ Theft ~
## 2 dcfa16f~ 2020~ City o~ City o~ -0.0941    51.5 On or ~ E01000~ City o~ Theft ~
## 3 be9310e~ 2020~ City o~ City o~ -0.0945    51.5 On or ~ E01000~ City o~ Theft ~
## 4 0cbb0c5~ 2020~ City o~ City o~ -0.0945    51.5 On or ~ E01000~ City o~ Theft ~
## 5 aaafbcf~ 2020~ City o~ City o~ -0.0750    51.5 On or ~ E01000~ City o~ Theft ~
## 6 8249cc1~ 2020~ City o~ City o~ -0.0750    51.5 On or ~ E01000~ City o~ Theft ~
## # ... with 2 more variables: last_outcome_category <chr>, context <lgl>, and
## #   abbreviated variable names 1: reported_by, 2: falls_within, 3: longitude,
## #   4: latitude, 5: location, 6: lsoa_code, 7: lsoa_name, 8: crime_type
\end{verbatim}

You should see these rows display in your console. Great, the data set looks as we remember, with the different fields, including, importantly for this week, the \textbf{LSOA code} which we will use to process and join our data together.

Next, let's add our \texttt{ward\_population\_2019.shp}. We will store this as a variable called \texttt{ward\_population} and use the \texttt{sf} library to load the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in our ward\_population\_2019 shp from our output data folder}
\NormalTok{ward\_population }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"data/output/ward\_population\_2019.shp"}\NormalTok{, }\AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Reading layer `ward_population_2019' from data source 
##   `/Users/justinvandijk/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0030/data/output/ward_population_2019.shp' 
##   using driver `ESRI Shapefile'
## Simple feature collection with 657 features and 7 fields
## Geometry type: POLYGON
## Dimension:     XY
## Bounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9
## Projected CRS: OSGB36 / British National Grid
\end{verbatim}

You should now see the \texttt{ward\_population} variable appear in your environment window.

\hypertarget{interacting-with-spatial-data}{%
\subsection{Interacting with spatial data}\label{interacting-with-spatial-data}}

As this is the first time we have loaded spatial data into R, let's go for a little exploration of how we can interact with our spatial data frame. The first thing we want to do when we load spatial data is, of course, map it to if everything is in order. To do this, we can use a really simple command from R's \texttt{base} library: \texttt{plot()}. As we do not necessarily want to plot this data everytime we run this script in the future, we can type this command into the console:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot our ward\_population data}
\FunctionTok{plot}\NormalTok{(ward\_population)}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-plot-wardpop-1.pdf}

You should see your \texttt{ward\_population} plot appear in your \textbf{Plots} window - as you will see, your ward data set is plotted `thematically' by each of the fields within the data set, including our \texttt{pop2019} field we created last week.

\textbf{Tip}
This \texttt{plot()} function is not to be used to make maps but can be used as a quick way of viewing our spatial data.

We can also find out more information about our \texttt{ward\_population} data. Let's next check out our class of our data. Again, \textbf{in the console} type:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find out the class of our ward\_population data}
\FunctionTok{class}\NormalTok{(ward\_population)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sf"         "data.frame"
\end{verbatim}

We should see our data is an \texttt{sf} dataframe, which is great as it means we can utilise our \texttt{tidyverse} libraries with our \texttt{ward\_population}. We can also use the \texttt{attributes()} function we looked at last week to find out a little more about the spatial part of our data frame:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find out the attributes of our ward\_population data}
\FunctionTok{attributes}\NormalTok{(ward\_population)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $names
## [1] "NAME"       "GSS_CODE"   "DISTRICT"   "LAGSSCODE"  "HECTARES"  
## [6] "NONLD_AREA" "pop2019"    "geometry"  
## 
## $row.names
##   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
##  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
##  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
##  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
##  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
##  [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
## [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
## [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144
## [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162
## [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180
## [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198
## [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216
## [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234
## [235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252
## [253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270
## [271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288
## [289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306
## [307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324
## [325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342
## [343] 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360
## [361] 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378
## [379] 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396
## [397] 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414
## [415] 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432
## [433] 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450
## [451] 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468
## [469] 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486
## [487] 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504
## [505] 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522
## [523] 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540
## [541] 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558
## [559] 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576
## [577] 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594
## [595] 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612
## [613] 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630
## [631] 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648
## [649] 649 650 651 652 653 654 655 656 657
## 
## $class
## [1] "sf"         "data.frame"
## 
## $sf_column
## [1] "geometry"
## 
## $agr
##       NAME   GSS_CODE   DISTRICT  LAGSSCODE   HECTARES NONLD_AREA    pop2019 
##       <NA>       <NA>       <NA>       <NA>       <NA>       <NA>       <NA> 
## Levels: constant aggregate identity
\end{verbatim}

We can see how many rows we have, the names of our rows and a few more pieces of information about our \texttt{ward\_population} data, for example, we can see that the specific \texttt{\$sf\_column} i.e.~our spatial information) in our data set is called \texttt{geometry}.

We can investigate this column a little more by \textbf{selecting} this column within our console to return. In the \textbf{console} type:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get info about the geometry of our ward\_population data}
\NormalTok{ward\_population}\SpecialCharTok{$}\NormalTok{geometry}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Geometry set for 657 features 
## Geometry type: POLYGON
## Dimension:     XY
## Bounding box:  xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9
## Projected CRS: OSGB36 / British National Grid
## First 5 geometries:
\end{verbatim}

You should see new information about our \texttt{geometry} column display in your console.

From this selection we can find out the dataset's:

\begin{itemize}
\tightlist
\item
  geometry type
\item
  dimension
\item
  bbox (bounding box)
\item
  CRS (coordinate reference system)
\end{itemize}

And also the first five geometries of our data set.

This is really useful as one of the first things we want to know about our spatial data is what \emph{coordinate system} it is projected with. As we should know, our \texttt{ward\_population} data was created and exported within \emph{British National Grid}, therefore seeing the EPSG code of British National Grid - 27700 - as our CRS confirms to us that R has read in our data set correctly.

We could also actually find out this information using the \texttt{st\_crs()} function from the \texttt{sf} library.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find out the CRS of our ward\_population data}
\FunctionTok{st\_crs}\NormalTok{(ward\_population)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Coordinate Reference System:
##   User input: OSGB36 / British National Grid 
##   wkt:
## PROJCRS["OSGB36 / British National Grid",
##     BASEGEOGCRS["OSGB36",
##         DATUM["Ordnance Survey of Great Britain 1936",
##             ELLIPSOID["Airy 1830",6377563.396,299.3249646,
##                 LENGTHUNIT["metre",1]]],
##         PRIMEM["Greenwich",0,
##             ANGLEUNIT["degree",0.0174532925199433]],
##         ID["EPSG",4277]],
##     CONVERSION["British National Grid",
##         METHOD["Transverse Mercator",
##             ID["EPSG",9807]],
##         PARAMETER["Latitude of natural origin",49,
##             ANGLEUNIT["degree",0.0174532925199433],
##             ID["EPSG",8801]],
##         PARAMETER["Longitude of natural origin",-2,
##             ANGLEUNIT["degree",0.0174532925199433],
##             ID["EPSG",8802]],
##         PARAMETER["Scale factor at natural origin",0.9996012717,
##             SCALEUNIT["unity",1],
##             ID["EPSG",8805]],
##         PARAMETER["False easting",400000,
##             LENGTHUNIT["metre",1],
##             ID["EPSG",8806]],
##         PARAMETER["False northing",-100000,
##             LENGTHUNIT["metre",1],
##             ID["EPSG",8807]]],
##     CS[Cartesian,2],
##         AXIS["(E)",east,
##             ORDER[1],
##             LENGTHUNIT["metre",1]],
##         AXIS["(N)",north,
##             ORDER[2],
##             LENGTHUNIT["metre",1]],
##     USAGE[
##         SCOPE["Engineering survey, topographic mapping."],
##         AREA["United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore."],
##         BBOX[49.75,-9,61.01,2.01]],
##     ID["EPSG",27700]]
\end{verbatim}

You notice that we actually get a lot more information about our CRS beyond simply the code using this function. This function is really important to us as users of spatial data as it allows us to retrieve and set the CRS of our spatial data when the data does not come with a \texttt{.proj} file but we do know what projection system should be used.

The final thing we might want to do before we get started with our data analysis is to simply look at the data table part of our data set, i.e.~what we called the \textbf{Attribute Table} in QGIS, but here it is simply the table part of our data frame. To do so, you can either use the \texttt{View()} function in the console or click on the \texttt{ward\_population} variable within our environment.

\hypertarget{getting-our-crime-data-in-shape}{%
\subsection{Getting our crime data in shape}\label{getting-our-crime-data-in-shape}}

Now we have our data loaded, our next step is to process our data to create what we need as our final output for analysis: a spatial dataframe that contains a \textbf{theft crime rate} for each ward for each month in 2020. However, if we look at our \texttt{all\_theft\_df}, we do not have a field that contains the wards our crimes have occurred in. We only have two types of spatial or spatially-relevant data in our \texttt{all\_theft\_df}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  The approximate WGS84 \textbf{latitude} and \textbf{longitude}.
\item
  The \textbf{Lower Super Output Area (LSOA)} in which the crime it occurred.
\end{enumerate}

From Week 3's practical, we know we can map our points using the coordinates and then provide a count by using a \textbf{point-in-polygon} (PIP) operation. However to do this for each month, we would need to filter our data set for each month and repeat the PIP operation - when we know a little more advanced code, this might end up being quite simple, but for now we will try to see if we can solve it differently.

\hypertarget{attribute-join}{%
\subsubsection{Attribute join}\label{attribute-join}}

To get the number of crimes that occurred in each ward, all we need to do is figure our which Ward our LSOAs fall within and then we can add this as an additional attribute or rather \textbf{column} to our \texttt{all\_theft\_df}.

From a GIScience perspective, there are many ways to do this but the most straight forward is to use something called a \textbf{look-up table}. Look-up tables are an extremely common tool in database management and programming, providing a very simple approach to storing additional information about a feature (such as a row within a dataframe) in a separate table that can quite literally be ``looked up'' when needed for a specific application.

In our case, we will actually join our look-up table to our current \texttt{all\_theft\_df} to get this information ``hard-coded'' to our dataframe for ease of use. To be able to do this, we therefore need to find a look-up table that contains a list of \textbf{LSOAs} in London and the \textbf{wards} in which they are contained. Lucky for us the \href{https://www.ons.gov.uk/}{Office for National Statistics} provides this for us in their Open Geography Portal. They have a table that contains exactly what we're looking for: \textbf{Lower Layer Super Output Area (2011) to Ward (2018) Lookup in England and Wales v3}. As the description on the website tells us: ``\emph{This file is a best-fit lookup between 2011 lower layer super output areas, electoral wards/divisions and local authority districts in England and Wales as at 31 December 2018.}''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Download the look-up table from the ONS: \href{https://geoportal.statistics.gov.uk/datasets/8c05b84af48f4d25a2be35f1d984b883_0}{Link}
\item
  Move this file in your \textbf{data} -\textgreater{} \textbf{raw} -\textgreater{} \textbf{boundaries} folder and rename to \texttt{data/raw/boundaries/lsoa\_ward\_lookup.csv}.
\item
  Load the dataset using the \texttt{read\_csv()} function. Do not worry if you have a few ``parsing'' failures, the table should still work fine.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in our lsoa\_ward\_lookup csv from our raw boundaries data folder}
\NormalTok{lsoa\_ward\_lookup }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/raw/boundaries/lsoa\_ward\_lookup.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we have our lookup table, all we are going to do is extract the relevant ward name and code for each of the LSOAs in our \texttt{all\_theft\_df}. To do so, we're going to use one of the \texttt{join} functions from the \texttt{dplyr} library.

\textbf{Note}
We have already learnt how to complete \textbf{Attribute Joins} in QGIS via the \textbf{Joins} tab in the Properties window so it should come of no surprise that we can do exactly the same process within R. To conduct a join between two dataframes (spatial or non-spatial, it does not matter), we use the same principles of selecting a \textbf{unique but matching} field within our dataframes to join them together.

Within R, you have two options to complete a data frame join:

\begin{itemize}
\tightlist
\item
  The \textbf{first} is to use the \texttt{base} R library and its \texttt{merge()} function:

  \begin{itemize}
  \tightlist
  \item
    By default the data frames are merged on the columns with names they both have, but you can also provide the columns to match separate by using the parameters: \texttt{by.x} and \texttt{by.y}.
  \item
    Your code would look something like: \texttt{merge(x,\ y,\ by.x\ =\ "xColName",\ by.y\ =\ "yColName")}, with \texttt{x} and \texttt{y} each representing a dataframe.
  \item
    The rows in the two data frames that match on the specified columns are extracted, and joined together.
  \item
    If there is more than one match, all possible matches contribute one row each, but you can also tell merge whether you want all rows, including ones without a match, or just rows that match, with the arguments all.x and all.
  \end{itemize}
\item
  The \textbf{second} option is to use the \texttt{dplyr} library:

  \begin{itemize}
  \tightlist
  \item
    \texttt{dplyr} uses \href{https://en.wikipedia.org/wiki/SQL}{SQL} database syntax for its join functions.
  \item
    There are \textbf{four types} of joins possible with the \texttt{dplyr} library.

    \begin{itemize}
    \tightlist
    \item
      \texttt{inner\_join()}: includes all rows that exist both within \texttt{x} and \texttt{y.}
    \item
      \texttt{left\_join()}: includes all rows in \texttt{x.}
    \item
      \texttt{right\_join()}: includes all rows in \texttt{y.}
    \item
      \texttt{full\_join()}: includes all rows in \texttt{x} and \texttt{y}.
    \end{itemize}
  \item
    Figuring out which one you need will be on a case by case basis.
  \item
    Again, if the join columns have the same name, all you need is \texttt{left\_join(x,\ y)}.
  \item
    If they do not have the same name, you need a \texttt{by} argument, such as \texttt{left\_join(x,\ y,\ by\ =\ c("xName"\ =\ "yName"))}. Left of the equals is the column for the first data frame, right of the equals is the name of the column for the second data frame.
  \end{itemize}
\end{itemize}

As we have seen from the list of fields above, we know that we have at least \textbf{two} fields that should match across the data sets: our \textbf{lsoa codes} and \textbf{lsoa names}. We of course need to identify the precise fields that contain these values in each of our data frames, i.e.~\texttt{LSOA11CD} and \texttt{LSOA11NM} in our \texttt{lsoa\_ward\_lookup} dataframe and \texttt{lsoa\_code} and \texttt{lsoa\_name} in our \texttt{all\_theft\_df} dataframe, but once we know what fields we can use, we can go ahead and join our two data frames together.

We are going to need to make \textbf{multiple} joins between our tables as we have multiple entries of crime for the same LSOA. In addition, we are going to need to use a function that allows us to keep \textbf{all rows} in our \texttt{all\_theft\_df} dataframe, but we do not need to keep \textbf{all rows} in our \texttt{lsoa\_ward\_lookup} if those wards are not within our data set.

Let's have a look in detail at how the four different types of joins from \texttt{dplyr} work:

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w05/dplyr-joins} 

}

\caption{Different types of joins.}\label{fig:05-dplyr}
\end{figure}

It looks like we're going to need to use our \texttt{left\_join()} function as we want to join matching rows from our \texttt{lsoa\_ward\_lookup} dataframe to our \texttt{all\_theft\_df} dataframe but make sure to keep all rows in the latter. Create a join between our two dataframes and store as a new variable:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# join lsoa\_ward\_lookup rows to the all\_theft\_df on our two lsoa code fields}
\NormalTok{all\_theft\_ward\_df }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(all\_theft\_df, lsoa\_ward\_lookup, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"lsoa\_code"} \OtherTok{=} \StringTok{"LSOA11CD"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let's go ahead and check our join to make sure that our LSOA codes and names match across our new dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# check our join via the first five rows}
\FunctionTok{head}\NormalTok{(all\_theft\_ward\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 19
##   crime_id month repor~1 falls~2 longi~3 latit~4 locat~5 lsoa_~6 lsoa_~7 crime~8
##   <chr>    <chr> <chr>   <chr>     <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>  
## 1 37c663d~ 2020~ City o~ City o~ -0.106     51.5 On or ~ E01000~ Camden~ Theft ~
## 2 dcfa16f~ 2020~ City o~ City o~ -0.0941    51.5 On or ~ E01000~ City o~ Theft ~
## 3 be9310e~ 2020~ City o~ City o~ -0.0945    51.5 On or ~ E01000~ City o~ Theft ~
## 4 0cbb0c5~ 2020~ City o~ City o~ -0.0945    51.5 On or ~ E01000~ City o~ Theft ~
## 5 aaafbcf~ 2020~ City o~ City o~ -0.0750    51.5 On or ~ E01000~ City o~ Theft ~
## 6 8249cc1~ 2020~ City o~ City o~ -0.0750    51.5 On or ~ E01000~ City o~ Theft ~
## # ... with 9 more variables: last_outcome_category <chr>, context <lgl>,
## #   LSOA11NM <chr>, WD18CD <chr>, WD18NM <chr>, WD18NMW <chr>, LAD18CD <chr>,
## #   LAD18NM <chr>, FID <dbl>, and abbreviated variable names 1: reported_by,
## #   2: falls_within, 3: longitude, 4: latitude, 5: location, 6: lsoa_code,
## #   7: lsoa_name, 8: crime_type
\end{verbatim}

You should now see that you have with \textbf{19} variables: \textbf{12} from \texttt{all\_theft\_df}, plus \textbf{7} from \texttt{lsoa\_ward\_lookup.} Now we have our joined data set, we can move forward with some more data wrangling. Before we do this, it would be good if we could trim down our dataframe to only the relevant data that we need moving forward.

To be able to `trim' our data frame, we have two choices in terms of the code we might want to run. First, we could look to drop certain columns from our data frame. Alternatively, we could create a subset of the columns we want to keep from our data frame and store this as a new variable or simply overwrite the currently stored variable. To do either of these types of data transformation, we need to know more about how we can interact with a data frame in terms of \textbf{indexing}, \textbf{selecting} and \textbf{slicing}.

\hypertarget{data-wrangling}{%
\subsection{Data wrangling}\label{data-wrangling}}

Everything we will be doing today as we progress with our data frame cleaning and processing relies on us understanding how to interact with and transform our data frame. This interaction itself relies on knowing about how \textbf{indexing} works in R as well as how to \textbf{select} and \textbf{slice} your data frame to extract the relevant cells, rows or columns and then manipulate them. Whilst there are traditional programming approaches to this using the base R library, \texttt{dplyr} is making this type of data wrangling much easier. The following video provides an excellent explanation from both a \texttt{base} R perspective as well as using the \texttt{dplyr} library. It also includes a good explanation about what our pipe function , \texttt{\%\textgreater{}\%} or \texttt{\textbar{}\textgreater{}} , does.

As you can see from the video, there are two common approaches to selection and slicing in R, which rely on indexing and/or field names in different ways.

\hypertarget{selection-with-base-r}{%
\subsubsection{Selection with base R}\label{selection-with-base-r}}

The most basic approach to selecting and slicing within programming relies on the principle of using \textbf{indexes} within our data structures. Indexes actually apply to any type of data structure, from single atomic vectors to complicated data frames as we use here. Indexing is the numbering associated with each element of a data structure. For example, if we create a simple vector that stores several strings:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# store a simple vector of strings}
\NormalTok{simple\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Aa"}\NormalTok{, }\StringTok{"Bb"}\NormalTok{, }\StringTok{"Cc"}\NormalTok{, }\StringTok{"Dd"}\NormalTok{, }\StringTok{"Ee"}\NormalTok{, }\StringTok{"Ff"}\NormalTok{, }\StringTok{"Gg"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

R will assign each element (i.e.~string) within this simple vector with a number: \texttt{Aa} = 1, \texttt{Bb} = 2, \texttt{Cc} = 3 and so on. Now we can go ahead and select each element by using the base selection syntax which is using square brackets after your element's variable name, as so:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the first element of our variable}
\NormalTok{simple\_vector[}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Aa"
\end{verbatim}

Which should return the first element, our first string containing \texttt{Aa}. You could change the number in the square brackets to any number up to 7 and you would return each specific element in our vector. However, say you do not want the first element of our vector but the second to fifth elements. To achieve this, we conduct what is known in programming as a \textbf{slicing} operation, where, using the \texttt{{[}{]}} syntax, we add a colon \texttt{:} to tell R where to \textbf{start} and where to \textbf{end} in creating a selection, known as a \textbf{slice}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the second to fifth element of our vector, creating a \textquotesingle{}slice\textquotesingle{} of our vector}
\NormalTok{simple\_vector[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Bb" "Cc" "Dd" "Ee"
\end{verbatim}

You should now see our 2nd to 5th elements returned. Now what is super cool about selection and slicing is that we can add in a simple \textbf{- (minus)} sign to essentially reverse our selection. So for example, we want to return everything \textbf{but} the 3rd element:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select everything but the third element of our vector}
\NormalTok{simple\_vector[}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Aa" "Bb" "Dd" "Ee" "Ff" "Gg"
\end{verbatim}

And with a slice, we can use the minus to slice \textbf{out} parts of our vector, for example, remove the 2nd to the 5th elements (note the use of a minus sign for \textbf{both}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the second to fifth element of our vector, creating a \textquotesingle{}slice\textquotesingle{} of our vector}
\NormalTok{simple\_vector[}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{:{-}}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Aa" "Ff" "Gg"
\end{verbatim}

\textbf{Note}
This use of \textbf{square brackets} for selection syntax is common across many programming languages, including Python, but there are often some differences you will need to be aware of if you pursue other languages. For example:

\begin{itemize}
\tightlist
\item
  Python always starts its index from \texttt{0}! Whereas we can see here with R, our index starts at \texttt{1}.
\item
  R is unable to index the characters within strings. This is something you can do in Python, but in R, we will need to use a function such as \texttt{substring()}.
\end{itemize}

We can also apply these selection techniques to data frames, but we will have a little more functionality as our data frames are made from both \textbf{rows} and \textbf{columns}. This means when it comes to selections, we can utilise an amended selection syntax that follows a specific format to select individual rows, columns, slices of each, or just a single cell: \texttt{{[}rows,\ columns{]}}

There are many ways we can use this syntax, which we will show below using our \texttt{lsoa\_ward\_lookup} data frame. First, before looking through and executing these examples familiarise yourself with the \texttt{lsoa\_ward\_lookup} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# view lsoa\_ward\_lookup dataframe}
\FunctionTok{View}\NormalTok{(lsoa\_ward\_lookup)}
\end{Highlighting}
\end{Shaded}

To select a single column from your data frame, you can use one of two approaches. First we can follow the syntax above carefully and simply set our column parameter in our syntax above to the number 2:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the 2nd column from the data frame}
\NormalTok{lsoa\_ward\_lookup[,}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

You should see your second column display in your console. Second, we can actually select our column by only typing in the number (no need for the comma). By default, when there is only \textbf{one} argument present in the selection brackets, R will select the column from the data frame, not the row:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the 2nd column from the data frame}
\NormalTok{lsoa\_ward\_lookup[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

To \textbf{select a specific row}, we need to add in a comma after our number:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the 2nd row from the data frame}
\NormalTok{lsoa\_ward\_lookup[}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

You should see your second row appear. Now, to select a specific cell in our data frame, we simply provide both arguments in our selection parameters:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the value at the 2nd row and 2nd column in the data frame}
\NormalTok{lsoa\_ward\_lookup[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

What is also helpful in R is that we can select our columns by their field names by passing these field names to our selection brackets as a string. For a single column:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the LSOA11NM column (2nd column) by name}
\NormalTok{lsoa\_ward\_lookup[}\StringTok{"LSOA11NM"}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Or for more than one columns, we can supply a combined vector:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the LSOA11CD (1st column) and LSOA11NM column (2nd column) by name}
\NormalTok{lsoa\_ward\_lookup[}\FunctionTok{c}\NormalTok{(}\StringTok{"LSOA11CD"}\NormalTok{, }\StringTok{"LSOA11NM"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

To retrieve our 2nd - 4th columns in our data frame, we can use:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the 2nd to 4th columns from our data frame}
\NormalTok{lsoa\_ward\_lookup[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}

\CommentTok{\# select the 2nd to 4th columns from our data frame}
\NormalTok{ lsoa\_ward\_lookup[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

We can also apply the negative:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select everything but the 2nd to 4th columns from our data frame}
\NormalTok{lsoa\_ward\_lookup[}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{:{-}}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

If you do not want a slide, we can also provide a combined list of the columns we want to extract:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the 2nd, 3rd, 4th and 7th columns from our data frame}
\NormalTok{lsoa\_ward\_lookup[}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{7}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

We can apply this slicing approach to our rows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select the 2nd to 4th rows from our data frame}
\NormalTok{lsoa\_ward\_lookup[}\DecValTok{2}\SpecialCharTok{:}\DecValTok{4}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

As well as a negative selection:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select everything but the 2nd to 4th rows from our data frame}
\NormalTok{lsoa\_ward\_lookup[}\SpecialCharTok{{-}}\DecValTok{2}\SpecialCharTok{:{-}}\DecValTok{4}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\hypertarget{selection-with-dplyr}{%
\subsubsection{Selection with dplyr}\label{selection-with-dplyr}}

Instead of using the square brackets \texttt{{[}{]}} syntax, we now have functions that we can use to select or slice our data frames accordingly:

\begin{itemize}
\tightlist
\item
  For columns, we use the \texttt{select()} function that enables us to select one or more columns using their column names.
\item
  For rows, we use the \texttt{slice()} function that enables us to select one or more rows using their position (i.e.~similar to the process above).
\end{itemize}

For \textbf{both} functions, we can also use the negative \texttt{-} approach we saw in the base R approach to ``reverse a selection'', e.g.:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select column 2}
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(lsoa\_ward\_lookup, }\DecValTok{2}\NormalTok{)}

\CommentTok{\# select everything but column 2}
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(lsoa\_ward\_lookup, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)}

\CommentTok{\#select LSOA11CD column, note no ""}
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(lsoa\_ward\_lookup, LSOA11CD)}

\CommentTok{\# select everything but column 2, note no ""}
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(lsoa\_ward\_lookup, }\SpecialCharTok{{-}}\NormalTok{LSOA11CD)}

\CommentTok{\# select everything but column 2}
\NormalTok{dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(lsoa\_ward\_lookup, }\SpecialCharTok{{-}}\NormalTok{LSOA11CD)}
\end{Highlighting}
\end{Shaded}

In addition to these index-based functions, within \texttt{dplyr}, we also have \texttt{filter()} that enables us to easily filter rows within our data frame based on specific conditions (such as being a City of London ward). In addition, \texttt{dplyr} provides lots of functions that we can use directly with these selections to apply certain data wrangling processes to only specific parts of our data frame, such as \texttt{mutate()} or \texttt{count()}.

\textbf{Note}
We will be using quite a few of these functions in the remaining data wrangling section below - plus throughout our module, so it is highly recommend to download the \texttt{dplyr} \href{https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf}{cheat sheet} to keep track of what functions we are using and why!

As we have seen above, whilst there are two approaches to selection using either \texttt{base} R library or the \texttt{dplyr} library, we will continue to focus on using functions directly from the \texttt{dplyr} library to ensure efficiently and compatibility within our code. Within \texttt{dplyr}, as you also saw, whether we want to keep or drop columns, we always use the same function: \texttt{select()}.

To use this function, we provide our function with a single or list of the columns we want to keep or if we want to drop them, we use the same approach, but add a \texttt{-} before our selection. Let's see how we can extract just the relevant columns we will need for our future analysis. Note that we will overwrite our \texttt{all\_theft\_ward\_df} variable.

In your script, add the following code to extract only the relevant columns we need for our future analysis:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# reduce our data frame using the select function}
\NormalTok{all\_theft\_ward\_df }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(all\_theft\_ward\_df, crime\_id, month, longitude, latitude, lsoa\_name, lsoa\_code, crime\_type, WD18CD, WD18NM)}
\end{Highlighting}
\end{Shaded}

You should now see that your \texttt{all\_theft\_ward\_df} data frame should only contain nine variables. You can go and view this data frame or call the \texttt{head()} function on the data in the console if youlike to check out this new formatting.

\hypertarget{improving-your-workflow}{%
\subsection{Improving your workflow}\label{improving-your-workflow}}

Our current workflow looks good, we now have our data frame ready for use in wrangling but we could have done this a little more efficiently by using the pipe function \texttt{\%\textgreater{}\%} (or \texttt{\textbar{}\textgreater{}}). A pipe is used to push the outcome of one function/process into another. When ``piped'', we do not need to include the first ``data frame'' (or which data structure you are using) in the next function. The pipe ``automates'' this and pipes the results of the previous function directly into this function.

It might sound a little confusing at first, but once you start using it, it really can make your code quicker and easier to write and run and it stops us having to create lots of additional variables to store outputs along the way. It also enabled the code we used last week to load/read all the \texttt{csvs} at once.

In our workflow, we have so far:

\begin{itemize}
\tightlist
\item
  Joined our two data frames together.
\item
  Removed the columns not needed for our future analysis.
\end{itemize}

Let's see how we can combine this process into a single line of code:

\textbf{Option 1: Original code, added pipe}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# join, then select}
\NormalTok{all\_theft\_ward\_df\_speedy\_1 }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(all\_theft\_df, lsoa\_ward\_lookup, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"lsoa\_code"} \OtherTok{=} \StringTok{"LSOA11CD"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(crime\_id, month, longitude, latitude, lsoa\_name, lsoa\_code, crime\_type, WD18CD, WD18NM)}
\end{Highlighting}
\end{Shaded}

You should see that we now end up with a data frame akin to our final output above - the same number of observations and variables, all from one line of course. We could also take another approach in writing code, by completing our selection \textbf{prior} to our join, which would mean having to write out fewer field names when piping this output into our join:

\textbf{Option 2: New code, remove columns first}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# select, then join}
\NormalTok{all\_theft\_ward\_df\_speedy\_2 }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(lsoa\_ward\_lookup, LSOA11CD, WD18CD, WD18NM) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{right\_join}\NormalTok{(all\_theft\_df, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{( }\StringTok{"LSOA11CD"} \OtherTok{=} \StringTok{"lsoa\_code"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

What these two options do show is that there are multiple ways to achieve the same output, using similar code. Pipes help us improve the efficiency of our code. That being said: by adding the pipe, we are not able to check our join prior to the selection, so often it is better to add in improve the efficiency of your code once you are certain that your code has run correctly.

\hypertarget{aggregate-crime-by-ward}{%
\subsection{Aggregate crime by ward}\label{aggregate-crime-by-ward}}

To aggregate our crime by ward for each month in 2020, we need to use a combination of \texttt{dplyr} functions. First, we need to \textbf{group} our crime by each ward and then count - by month - the number of thefts occurring in each ward. To do so, we will use the \texttt{group\_by()} function and the \texttt{count()} function.

The \texttt{group\_by()} function creates a ``grouped'' copy of the table (in memory), then any \texttt{dplyr} function used on this grouped table will manipulate each group separately (i.e.~our count by month manipulation) and then \textbf{combine the results} to a single output.

If we solely run the \texttt{group\_by()} function, we won't really see this effect on its own, instead we need to add the \texttt{count()} function, which counts the number of rows in each group defined by the variables provided within the function, in our case, \texttt{month}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# group our crimes by ward, then count the number of thefts occurring in each month}
\NormalTok{theft\_count\_month\_ward }\OtherTok{\textless{}{-}} \FunctionTok{group\_by}\NormalTok{(all\_theft\_ward\_df, WD18CD) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{count}\NormalTok{(month)}
\end{Highlighting}
\end{Shaded}

To understand our output, go ahead and \texttt{View()} the variable. You should see that we have ended up with a new table that lists each ward (by the \texttt{WD18CD} column) twelve times, to detail the number of thefts for each month - with the months represented as a single field. What we would really prefer is to have our crime count detailed as one field for each individual month, i.e.~\texttt{2020-01} as a single field, then \texttt{2020-02}, etc.

To change the \emph{shape} of our data, we are going to need to use \texttt{tidyr}'s pivot functions. In the \texttt{tidyr} library, we have the choice of two \texttt{pivot()} functions: \texttt{pivot\_longer()} or \texttt{pivot\_wider()}.

\begin{itemize}
\tightlist
\item
  \texttt{pivot\_wider()} ``widens'' data, increasing the number of columns and decreasing the number of rows.
\item
  \texttt{pivot\_longer()} ``lengthens'' data, increasing the number of rows and decreasing the number of columns.
\end{itemize}

Our data is already pretty long, so that seems to suggest that we should use \texttt{pivot\_wider()}. We just need to first read through the documentation to figure out what parameters we need to use and how. Type \texttt{?pivot\_wider} into the console to access the documentation.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/w05/pivot_wider} 

}

\caption{Documentation for the `pivot_wider()` function.}\label{fig:05-pivot-wider}
\end{figure}

If we read through the documentation, we can figure our that our two parameters of interest are the \texttt{names\_from} and \texttt{values\_from} fields. We use the \texttt{names\_from} parameter to set our \texttt{month} column as the column from which to derive output fields from, and the \texttt{values\_from} field as our \texttt{n} field (count field) to set our values. As we do not have a field that uniquely identifies each of our rows, we can not use the \texttt{id\_cols} parameter. We will therefore need to state the parameters in our code to make sure the function reads in our fields for the right parameter:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pivot wider}
\NormalTok{theft\_by\_ward\_month\_df }\OtherTok{\textless{}{-}} \FunctionTok{pivot\_wider}\NormalTok{(theft\_count\_month\_ward, }\AttributeTok{names\_from =}\NormalTok{ month, }\AttributeTok{values\_from =}\NormalTok{ n)}

\CommentTok{\# inspect}
\NormalTok{theft\_by\_ward\_month\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 645 x 13
## # Groups:   WD18CD [645]
##    WD18CD    `2020-01` `2020-02` 2020-~1 2020-~2 2020-~3 2020-~4 2020-~5 2020-~6
##    <chr>         <int>     <int>   <int>   <int>   <int>   <int>   <int>   <int>
##  1 E05000026        30        28      25       6      12      13      11      10
##  2 E05000027         2         1       1       1       1      NA       3       1
##  3 E05000028         1         1       2       2       2       1      NA       1
##  4 E05000029        NA        NA      NA      NA       2      NA       2      NA
##  5 E05000030         3        NA      NA       1      NA       1      NA       3
##  6 E05000031         4        NA       4       2       1      NA       1       1
##  7 E05000032        NA        NA       2      NA       2       3       2       1
##  8 E05000033         1         2       5      NA      NA       1      NA       1
##  9 E05000034         1        NA       1      NA       1       2       1      NA
## 10 E05000035         3         4      NA      NA       4       2       3       3
## # ... with 635 more rows, 4 more variables: `2020-09` <int>, `2020-10` <int>,
## #   `2020-11` <int>, `2020-12` <int>, and abbreviated variable names
## #   1: `2020-03`, 2: `2020-04`, 3: `2020-05`, 4: `2020-06`, 5: `2020-07`,
## #   6: `2020-08`
\end{verbatim}

One final thing we want to do is clean up the names of our fields to mean a little more to us. Let's transform our numeric dates to text dates (and change our \texttt{WD18CD} in the process).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# rename columns}
\FunctionTok{names}\NormalTok{(theft\_by\_ward\_month\_df) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}ward\_code\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}jan\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}feb\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}mar\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}apr\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}may\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}jun\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}jul\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}aug\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}sept\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}oct\_2020\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}nov\_2020\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}dec\_2020\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Write out the completed theft table to a new \texttt{csv} file for future reference:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write out the theft\_crime\_df to a csv within our ouput data folder}
\FunctionTok{write.csv}\NormalTok{(theft\_by\_ward\_month\_df, }\StringTok{"data/output/theft\_by\_ward\_per\_month\_2020.csv"}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{joining-crime-data-to-wards}{%
\subsection{Joining crime data to wards}\label{joining-crime-data-to-wards}}

We are now getting to the final stages of our data processing, we just need to join our completed theft table, \texttt{theft\_by\_ward\_month\_df} to our \texttt{ward\_population} spatial data frame and then compute a theft crime rate. This will then allow us to map our theft rates per month by ward, exactly what we set to achieve within this practical. Luckily for us, the join approach we used earlier between our \texttt{all\_theft\_df} and our \texttt{lsoa\_ward\_lookup} is the exact same approach we need for this, even when dealing with spatial data.

Let's go ahead and use the same \texttt{left\_join()} function to join our two data frames together. In this case, we want to keep all rows in our \texttt{ward\_population} spatial data frame, so this will be our \texttt{x} data frame, whilst the \texttt{theft\_by\_ward\_month\_df} will be our \texttt{y}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# join theft by month to the correct wards in our ward\_population data frame}
\NormalTok{all\_theft\_ward\_sdf }\OtherTok{\textless{}{-}} \FunctionTok{left\_join}\NormalTok{(ward\_population, theft\_by\_ward\_month\_df, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"GSS\_CODE"} \OtherTok{=} \StringTok{"ward\_code"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To double-check our join, we want to do one extra step of \textbf{quality checks} and check that each of our wards has at least one occurence of crime over the twelve month period. We do this by computing a new column that totals the number of thefts. By identifying any wards that have zero entries (i.e.~\texttt{NA}s for each month), we can double-check with our original \texttt{theft\_by\_ward\_month\_df} to see if this is the correct ``data'' for that ward or if there has been an error in our join.

We can compute a new column by using the \texttt{mutate()} function from the \texttt{dplyr} library. We use the \texttt{rowsums()} function from the base library to compute the sum of rows and we use the \texttt{across()} function from the \texttt{dplyr} library to identify the columns for which we want to know the sum.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# total number of thefts for each ward, create new column}
\NormalTok{all\_theft\_ward\_sdf }\OtherTok{\textless{}{-}}\NormalTok{ all\_theft\_ward\_sdf }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{theft\_total =} \FunctionTok{rowSums}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\DecValTok{8}\SpecialCharTok{:}\DecValTok{19}\NormalTok{), }\AttributeTok{na.rm =}\NormalTok{ T))}
\end{Highlighting}
\end{Shaded}

You can now \texttt{View()} our updated \texttt{all\_theft\_ward\_sdf} spatial data frame and sort out columns to see those with a \texttt{theft\_total} of 0. What you should see is that we have approximately 20 City of London wards without data, whilst we do indeed have 10 additional wards without data. This seems not unlikely, so we can move forward with our data set as it is, but what we will need to do is adjust the values present within these wards prior to our visualisation analysis: these should not have \texttt{NA} as their value but rather \texttt{0.} In comparison our City of London wards should only contain \texttt{NA}. To make sure our data is as correct as possible prior to visualisation, we will remove our City of London wards that do not have any data (crime or population), and then convert the \texttt{NA} in our theft counts to \texttt{0}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# filter out City of London wards with a crime count of 0 or a population of 0}
\NormalTok{all\_theft\_ward\_sdf }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(all\_theft\_ward\_sdf, theft\_total }\SpecialCharTok{\textgreater{}} \DecValTok{0} \SpecialCharTok{|}\NormalTok{ DISTRICT }\SpecialCharTok{!=} \StringTok{"City and County of the City of London"}\NormalTok{) }

\CommentTok{\# remove the ward of Vintry, which whilst it has a positive crime count, it does not contain a population}
\NormalTok{all\_theft\_ward\_sdf }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(all\_theft\_ward\_sdf, NAME }\SpecialCharTok{!=} \StringTok{"Vintry"}\NormalTok{)}

\CommentTok{\# replace all NAs in our data frame with 0}
\NormalTok{all\_theft\_ward\_sdf[}\FunctionTok{is.na}\NormalTok{(all\_theft\_ward\_sdf)] }\OtherTok{=} \DecValTok{0}
\end{Highlighting}
\end{Shaded}

The final step we need to take before we can map our theft data is, of course, compute a crime rate per month for our \texttt{all\_theft\_ward\_sdf} data frame. We have our \texttt{pop2019} column within our \texttt{all\_theft\_ward\_sdf} data frame - we just need to figure out the code that allows us to apply our calculation that we've used in our previous practicals (i.e.~using the Attribute/Field Calculator in QGIS: \textbf{value/pop2019 * 10000}) to each of our datasets.

Once again, after a bit of searching, we can find out that the \texttt{mutate()} function comes in handy and we can follow a specific approach in our code that allows us to apply the above equation to all of our columns within our data frame. Now this is certainly a big jump in terms of complexity of our code: we are going to store within our \texttt{crime\_rate} variable our own function that calculates crime rate on a given value, currently called \texttt{x}. We will then apply this function on each relevant cell within our \texttt{all\_theft\_ward\_sdf} using the \texttt{mutate\_at()} function:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a new function called crime rate, which takes an argument, x, and the crime rate calculation}
\NormalTok{crime\_rate }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(x, }\AttributeTok{na.rm =} \ConstantTok{FALSE}\NormalTok{) ((x}\SpecialCharTok{/}\NormalTok{all\_theft\_ward\_sdf}\SpecialCharTok{$}\NormalTok{pop2019)}\SpecialCharTok{*}\DecValTok{10000}\NormalTok{)}

\CommentTok{\# apply this calculation to all columns between 8 and 19 within the all\_theft\_ward\_sdf and transform the values}
\NormalTok{theft\_crime\_rate\_sdf }\OtherTok{\textless{}{-}} \FunctionTok{mutate\_at}\NormalTok{(all\_theft\_ward\_sdf, }\FunctionTok{vars}\NormalTok{(}\DecValTok{8}\SpecialCharTok{:}\DecValTok{19}\NormalTok{), crime\_rate)}
\end{Highlighting}
\end{Shaded}

Have a look at your new \texttt{theft\_crime\_rate\_sdf} spatial data frame. Does it look as you would expect? Now we have our \textbf{final} data frame, we can go ahead and make our maps.

\hypertarget{mapping-crime-data}{%
\subsection{Mapping crime data}\label{mapping-crime-data}}

For making our maps, we will be using one of two main visualisation libraries that can be used for spatial data: \texttt{tmap}. \texttt{tmap} is a library written around thematic map visualisation. The package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps. What is really great about \texttt{tmap} is that it comes with one quick plotting method for a map called: \texttt{qtm()}.

We can use this function to plot the theft crime rate for one of our months really quickly. Within your script, use the \texttt{qtm} function to create a map of theft crime rate in London in January 2020.

\textbf{Note}
Before continuing do confirm whether your \texttt{theft\_crime\_rate\_sdf} is indeed still of class \texttt{sf}. In some instances it is possible that this changed when manipulating the attributes. You can simply check this by running \texttt{class(theft\_crime\_rate\_sdf)}. If your dataframe is not of class \texttt{sf}, you can force it into one by running \texttt{theft\_crime\_rate\_sdf\ \textless{}-\ st\_as\_sf(theft\_crime\_rate\_sdf))}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# quick thematic map for January 2020}
\FunctionTok{qtm}\NormalTok{(theft\_crime\_rate\_sdf, }\AttributeTok{fill=}\StringTok{"jan\_2020"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-jan-2020-cr-1.pdf}

In this case, the \texttt{fill} argument is how we tell \texttt{tmap} to create a choropleth map based on the values in the column we provide it with. If we simply set it to \texttt{NULL}, we would only draw the borders of our polygons. Within our \texttt{qtm} function, we can pass quite a few different parameters that would enable us to change specific aesthetics of our map - if you go ahead and look up the documentation for the function, you will see a list of these parameters. We can, for example, set the lines of our ward polygons to white by adding the \texttt{borders} parameter:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# quick thematic map for January 2020, include white borders}
\FunctionTok{qtm}\NormalTok{(theft\_crime\_rate\_sdf, }\AttributeTok{fill=}\StringTok{"jan\_2020"}\NormalTok{, }\AttributeTok{borders =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-jan-2020-cr-white-1.pdf}

The map does not really look great. We can continue to add and change parameters in our \texttt{qtm()} function to create a map we are satisfied with. However, the issue with the \texttt{qtm()} function is that it is quite limited in its functionality and mostly used to quickly inspect your data. Instead, when we want to develop more complex maps using the \texttt{tmap} library, we want to use their main plotting method which uses a function called \texttt{tm\_shape()}, which we build on using the \href{https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149}{layered grammar of graphics} approach.

\textbf{Tip}
When it comes to setting colours within a map or any graphic, we can either pass through a colour as a \textbf{word}, a \textbf{HEX code} or a pre-defined \textbf{palette}. You can find out more \href{http://www.sthda.com/english/wiki/colors-in-r}{here}, which is a great quick reference to just some of the possible colours and palettes you will be able to use in R.

The main approach to creating maps in \texttt{tmap} is to use the \href{https://towardsdatascience.com/a-comprehensive-guide-to-the-grammar-of-graphics-for-effective-visualization-of-multi-dimensional-1f92b4ed4149}{grammar of graphics} to build up a map based on what is called the \texttt{tm\_shape()} function. Essentially this function, when populated with a spatial data frame, takes the spatial information of our data (including the projection and geometry of our data) and creates a spatial ``object''. This object contains some information about our original spatial data frame that we can override (such as the projection) within this function's parameters, but ultimately, by using this function, you are instructing R that this is the object from which to ``draw my shape''.

To actually draw the shape, we next need to add a layer to specify the type of shape we want R to draw from this information - in our case, our polygon data. We need to add a function therefore that tells R to ``draw my spatial object as X'' and within this ``layer'', you can also specific additional information to tell R how to draw your layer. You can then add in additional layers, including other spatial objects (and their related shapes) that you want drawn on your map, plus a specify your layout options through a layout layer.

Let's see how we can build up our first map in \texttt{tmap}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+}
  \CommentTok{\# draw out spatial objects as polygons}
  \FunctionTok{tm\_polygons}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

As you should now see, we have now mapped the spatial polygons of our \texttt{theft\_crime\_rate\_sdf} spatial data frame. However, this is not the map we want: we want to have our polygons represented by a choropleth map where the colours reflect the theft crime rate in January, rather than the default grey polygons we see before us. To do so, we use the \texttt{col=} parameter that is within our \texttt{tm\_polygons()} shape.

\textbf{Tip}
The \texttt{col} parameter within \texttt{tm\_polygons()} is used to fill our polygons with a specific fill type, of either:

\begin{itemize}
\tightlist
\item
  a single color value (e.g.~\texttt{red})
\item
  the name of a data variable that is contained in the spatial data file Either the data variable contains color values, or values (numeric or categorical) that will be depicted by a specific color palette.
\item
  \texttt{MAP\_COLORS}. In this case polygons will be colored such that adjacent polygons do not get the same color.
\end{itemize}

Let's go ahead and pass our \texttt{jan\_2020} column within the \texttt{col=} parameter and see what we get:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+}
  \CommentTok{\# draw out spatial objects as polygons, specifying a data column}
  \FunctionTok{tm\_polygons}\NormalTok{(}\AttributeTok{col =} \StringTok{"jan\_2020"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-choro-1-1.pdf}

We are slowly getting there. But there are two things we can notice straight away that do not look right about our data. The first is that our \textbf{classification breaks} do not really reflect the variation in our dataset. This is because \texttt{tmap} has defaulted to its default break type: \textbf{pretty breaks}, whereas, as we know, using an approach such as \textbf{natural} breaks, aka \textbf{jenks}, may reveal better variation in our data.

Using the documentation for \texttt{tm\_polygons()}, it looks like the following parameters are relevant to help us create the right classification for our map:

\begin{itemize}
\tightlist
\item
  \texttt{n}: state the number of classification breaks you want.
\item
  \texttt{style}: state the style of breaks you want, e.g.~\texttt{fixed}, \texttt{sd}, \texttt{equal}, \texttt{quantile}.
\item
  \texttt{breaks}: state the numeric breaks you want to use when using the \textbf{fixed} style approach.
\end{itemize}

Let's say we want to change our choropleth map to have 5 classes, determined via the \texttt{jenks} method. We simply need to add the \texttt{n} and \texttt{style} parameters into our \texttt{tm\_polygons()} layer:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, specifying a data column, specifying jenks}
  \FunctionTok{tm\_polygons}\NormalTok{(}\AttributeTok{col =} \StringTok{"jan\_2020"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{style =} \StringTok{"jenks"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-choro-2-1.pdf}

We now have a choropleth that reflects better the distribution of our data, but we can make them a little prettier by rounding the values. To do so, we can change the \texttt{style} of the map to \texttt{fixed} and then supply a new argument for \texttt{breaks} that contains the rounded classification breaks:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, specifying a data column, specifying fixed breaks}
  \FunctionTok{tm\_polygons}\NormalTok{(}\AttributeTok{col =} \StringTok{"jan\_2020"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{style =} \StringTok{"fixed"}\NormalTok{, }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{434}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-choro-3-1.pdf}

That looks a little better from the classification side of things. We still have one final data-related challenge to solve, before we start to style our map and that is showing the polygons for City of London wards, even though we have no data for them. The easiest way to do so is to simply add a spatial object to our map that symbolises our polygons as grey wards and then draw the crime data on top:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set out tm\_shape equal to our original ward data frame}
\FunctionTok{tm\_shape}\NormalTok{(ward\_population) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, set to grey}
  \FunctionTok{tm\_polygons}\NormalTok{(}\StringTok{"gray"}\NormalTok{) }\SpecialCharTok{+}
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, specifying a data column, specifying fixed breaks}
  \FunctionTok{tm\_polygons}\NormalTok{(}\AttributeTok{col =} \StringTok{"jan\_2020"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{style =} \StringTok{"fixed"}\NormalTok{, }\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{434}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-choro-4-1.pdf}

\textbf{Note}
The order of your layers matters. What happens if you were to switch the layers around, i.e.~first add the crime rate layer and then the ``grey'' ward layer? Why?

\hypertarget{styling-crime-data}{%
\subsection{Styling crime data}\label{styling-crime-data}}

To style our map takes a further understanding and familiarity with our \texttt{tmap} library, but it is only something you will only really learn by having to make your own maps. As a result, we will not go into explaining \textbf{exactly} every aspect of map styling but instead provide you with some example code that you can use as well as experiment with to try to see how you can adjust aspects of the map to your preferences.

Fundamentally, the \textbf{key functions} to be aware of:

\begin{itemize}
\tightlist
\item
  \texttt{tm\_layout()}: contains parameters to style titles, fonts, the legend, etc.;
\item
  \texttt{tm\_compass()}: contains parameters to create and style a North arrow or compass;
\item
  \texttt{tm\_scale\_bar()}: contains parameters to create and style a scale bar.
\end{itemize}

To be able to start styling our map, we need to interrogate each of these functions and their parameters to trial and error options to ultimately create a map we are happy with:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set out tm\_shape equal to our original ward data frame}
\FunctionTok{tm\_shape}\NormalTok{(ward\_population) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, set to grey}
  \FunctionTok{tm\_polygons}\NormalTok{(}\StringTok{"gray"}\NormalTok{, }\AttributeTok{border.col =} \StringTok{"gray"}\NormalTok{) }\SpecialCharTok{+} 
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, specifying a data column, }
  \CommentTok{\# specifying fixed breaks, colour palette, and borders}
  \FunctionTok{tm\_polygons}\NormalTok{(}\AttributeTok{col =} \StringTok{"jan\_2020"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{style =} \StringTok{"fixed"}\NormalTok{, }
              \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{434}\NormalTok{), }
              \AttributeTok{palette =} \StringTok{"Blues"}\NormalTok{, }\AttributeTok{border.col =} \StringTok{"white"}\NormalTok{, }
              \AttributeTok{title =} \StringTok{"Rate per 10,000 people"}\NormalTok{) }\SpecialCharTok{+} 
  \CommentTok{\# add title}
  \FunctionTok{tm\_layout}\NormalTok{(}\AttributeTok{main.title =} \StringTok{\textquotesingle{}Theft Crime January 2020\textquotesingle{}}\NormalTok{, }
            \AttributeTok{main.title.fontface =} \DecValTok{2}\NormalTok{, }
            \AttributeTok{fontfamily =} \StringTok{"Helvetica"}\NormalTok{, }
            \AttributeTok{legend.outside =} \ConstantTok{TRUE}\NormalTok{, }
            \AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\StringTok{"left"}\NormalTok{,}\StringTok{"top"}\NormalTok{), }
            \AttributeTok{legend.title.size =} \DecValTok{1}\NormalTok{, }
            \AttributeTok{legend.title.fontface =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# add North arrow}
  \FunctionTok{tm\_compass}\NormalTok{(}\AttributeTok{type =} \StringTok{"arrow"}\NormalTok{, }
             \AttributeTok{position =} \FunctionTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }\SpecialCharTok{+} 
  \CommentTok{\# add scale bar}
  \FunctionTok{tm\_scale\_bar}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{), }
               \AttributeTok{position =} \FunctionTok{c}\NormalTok{(}\StringTok{"left"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{GEOG0030_files/figure-latex/05-map-template-1-1.pdf}

We will leave it at this, although there is a few more things we would want to do such as adding an additional legend property to state why the City of London wards are grey (\texttt{No\ Data}) as well as add our data source information.

\hypertarget{exporting-our-crime-data}{%
\subsection{Exporting our crime data}\label{exporting-our-crime-data}}

Once we are finished making our map, we can go ahead and export it to our \texttt{maps} folder. To do so, we need to save our map-making code to as a variable and then use the \texttt{tmap\_save()} function to save the output of this code to a picture within our maps folder.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add map object to variable}
\NormalTok{jan2020\_map }\OtherTok{\textless{}{-}} 
  \CommentTok{\# set out tm\_shape equal to our original ward data frame}
  \FunctionTok{tm\_shape}\NormalTok{(ward\_population) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, set to grey}
  \FunctionTok{tm\_polygons}\NormalTok{(}\StringTok{"gray"}\NormalTok{, }\AttributeTok{border.col =} \StringTok{"gray"}\NormalTok{) }\SpecialCharTok{+} 
\CommentTok{\# set our tm\_shape equal to our spatial data frame}
\FunctionTok{tm\_shape}\NormalTok{(theft\_crime\_rate\_sdf) }\SpecialCharTok{+} 
  \CommentTok{\# draw out spatial objects as polygons, specifying a data column, }
  \CommentTok{\# specifying fixed breaks, colour palette, and borders}
  \FunctionTok{tm\_polygons}\NormalTok{(}\AttributeTok{col =} \StringTok{"jan\_2020"}\NormalTok{, }\AttributeTok{n =} \DecValTok{5}\NormalTok{, }\AttributeTok{style =} \StringTok{"fixed"}\NormalTok{, }
              \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{118}\NormalTok{, }\DecValTok{434}\NormalTok{), }
              \AttributeTok{palette =} \StringTok{"Blues"}\NormalTok{, }\AttributeTok{border.col =} \StringTok{"white"}\NormalTok{, }
              \AttributeTok{title =} \StringTok{"Rate per 10,000 people"}\NormalTok{) }\SpecialCharTok{+} 
  \CommentTok{\# add title}
  \FunctionTok{tm\_layout}\NormalTok{(}\AttributeTok{main.title =} \StringTok{"Theft Crime January 2020"}\NormalTok{, }
            \AttributeTok{main.title.fontface =} \DecValTok{2}\NormalTok{, }
            \AttributeTok{fontfamily =} \StringTok{"Helvetica"}\NormalTok{, }
            \AttributeTok{legend.outside =} \ConstantTok{TRUE}\NormalTok{, }
            \AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\StringTok{"left"}\NormalTok{,}\StringTok{"top"}\NormalTok{), }
            \AttributeTok{legend.title.size =} \DecValTok{1}\NormalTok{, }
            \AttributeTok{legend.title.fontface =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \CommentTok{\# add North arrow}
  \FunctionTok{tm\_compass}\NormalTok{(}\AttributeTok{type =} \StringTok{"arrow"}\NormalTok{, }
             \AttributeTok{position =} \FunctionTok{c}\NormalTok{(}\StringTok{"right"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{)) }\SpecialCharTok{+} 
  \CommentTok{\# add scale bar}
  \FunctionTok{tm\_scale\_bar}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{20}\NormalTok{), }
               \AttributeTok{position =} \FunctionTok{c}\NormalTok{(}\StringTok{"left"}\NormalTok{, }\StringTok{"bottom"}\NormalTok{))}

\CommentTok{\# save as image}
\FunctionTok{tmap\_save}\NormalTok{(jan2020\_map, }\AttributeTok{filename =} \StringTok{"maps/jan2020\_theft\_crime\_map.png"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We also want to export the rest of our hard work in terms of data wrangling that we have completed for this practical - so let's go ahead and export our data frames so we can use them in future projects.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write out the all\_theft\_ward\_sdf to a shapefile within our working data folder}
\FunctionTok{st\_write}\NormalTok{(theft\_crime\_rate\_sdf,}\StringTok{"data/output/theft\_rate\_by\_ward\_per\_month\_2020.shp"}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# write out the all\_theft\_ward\_sdf to a shapefile within our working data folder}
\FunctionTok{st\_write}\NormalTok{(all\_theft\_ward\_sdf,}\StringTok{"data/output/theft\_count\_by\_ward\_per\_month\_2020.shp"}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\# write out the all\_theft\_ward\_sdf to a csv within our raw crime data folder}
\FunctionTok{write.csv}\NormalTok{(all\_theft\_ward\_sdf,}\StringTok{"data/output/theft\_count\_by\_ward\_per\_month\_2020.csv"}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{assignment-w05}{%
\section{Assignment}\label{assignment-w05}}

For your assignment for this week, we want you to create a map for a different month of 2020 with a different layout and look. Keep in mind:

\begin{itemize}
\tightlist
\item
  You will need to really think about your classification breaks when you change to map a different data set.
\item
  Play with the different settings, e.g.~change the colours of the map, change the legend title name, change the type of North arrow, etc.
\end{itemize}

If you are up for a challenge: now try to create a map in which you incorporate the maps of two different months together in one figure. Google is your friend!

\hypertarget{byl-w05}{%
\section{Before you leave}\label{byl-w05}}

And that is how you use R as a GIS in its most basic form. More RGIS in the coming weeks, but \href{https://www.youtube.com/watch?v=Ydg4T2MP7Z8}{this concludes the tutorial for this week}.

\end{document}
